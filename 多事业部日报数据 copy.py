#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šäº‹ä¸šéƒ¨æ—¥æŠ¥è‡ªåŠ¨åŒ–ä¸»æµç¨‹
- äº”å¤§äº‹ä¸šéƒ¨ã€äº”å¤§æ¸ é“ç‹¬ç«‹åˆ†ç»„
- æ•°æ®è¿‡æ»¤ï¼ˆåˆ·å•ã€è®¢å•çŠ¶æ€ã€çº¿ä¸Šåº—é“ºã€äº”å¤§æ¸ é“ï¼‰
- æŠ¥è¡¨ç”Ÿæˆã€webå‘å¸ƒã€å¾®ä¿¡æ¨é€
- æ¨é€å†…å®¹ç²¾ç®€ï¼Œwebé“¾æ¥å•ç‹¬ä¸€æ®µ
- ä¸¥æ ¼å¯¹é½æ•´ä½“æ—¥æŠ¥æ•°æ®.pyæœ€ä½³å®è·µ
"""

import os
import sys
import glob
import pandas as pd
import requests
from datetime import datetime, timedelta
import time
import traceback
import re
import unicodedata
import subprocess
import pymysql
import logging
import platform

# ========== æ—¥å¿—é…ç½® ==========
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = f"logs/å¤šäº‹ä¸šéƒ¨æ—¥æŠ¥æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

logger = setup_logging()

# ========== é…ç½®åŒº ==========
# æ•°æ®åº“é…ç½®
DB_HOST = "212.64.57.87"
DB_PORT = 3306
DB_USER = "root"
DB_PASSWORD = "c973ee9b500cc638"
DB_NAME = "Date"
DB_CHARSET = "utf8mb4"

# SSHéš§é“é…ç½®ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
SSH_HOST = "212.64.57.87"
SSH_USER = "root"
SSH_PORT = 22
LOCAL_PORT = 13306  # æœ¬åœ°è½¬å‘ç«¯å£

WECHAT_API = "http://212.64.57.87:5001/send"
WECHAT_TOKEN = "wecomchan_token"
WECHAT_USER = "weicungang"
EDGEONE_PROJECT = "sales-report"  # EdgeOne Pages é¡¹ç›®å
EDGEONE_TOKEN = "YxsKLIORJJqehzWS0UlrPKr4qgMJjikkqdJwTQ/SOYc="  # EdgeOne Pages API Token
REPORTS_DIR = "reports"

# å›ºå®šæ”¶ä»¶äººï¼ˆå‚è€ƒæ»é”€åº“å­˜æ¸…ç†è„šæœ¬ï¼‰
always_users = ["weicungang"]  # æµ‹è¯•æœŸé—´åªå‘é€ç»™ weicungang

# äº‹ä¸šéƒ¨é…ç½®ï¼ˆå®Œå…¨å‚è€ƒæ»é”€åº“å­˜æ¸…ç†è„šæœ¬çš„åˆ†ç»„é€»è¾‘ï¼‰
business_groups = {
    "ç©ºè°ƒäº‹ä¸šéƒ¨": {"keywords": ["ç©ºè°ƒ"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "åˆ¶å†·äº‹ä¸šéƒ¨": {"keywords": ["å†°ç®±","å†·æŸœ"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "æ´—æŠ¤äº‹ä¸šéƒ¨": {"keywords": ["æ´—è¡£æœº"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "æ°´è”ç½‘äº‹ä¸šéƒ¨": {"keywords": ["çƒ­æ°´å™¨", "å‡€æ°´", "é‡‡æš–", "ç”µçƒ­æ°´å™¨", "ç‡ƒæ°”çƒ­æ°´å™¨", "å¤šèƒ½æºçƒ­æ°´å™¨"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "å¨ç”µæ´—ç¢—æœºäº‹ä¸šéƒ¨": {"keywords": ["å¨ç”µ", "æ´—ç¢—æœº"], "users": ["weicungang"]}  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
}

# æ¸ é“åˆ†ç»„é…ç½®ï¼ˆæŒ‰åº—é“ºåç§°ä¸¥æ ¼ç­›é€‰ï¼‰
CHANNEL_GROUPS = {
    "å¡è¨å¸æ¸ é“": {"keywords": ["å¡è¨å¸", "å°çº¢ä¹¦"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "å¤©çŒ«æ¸ é“": {"keywords": ["å¤©çŒ«", "æ·˜å®"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "äº¬ä¸œæ¸ é“": {"keywords": ["äº¬ä¸œ"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "æ‹¼å¤šå¤šæ¸ é“": {"keywords": ["æ‹¼å¤šå¤š"], "users": ["weicungang"]},  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
    "æŠ–éŸ³æ¸ é“": {"keywords": ["æŠ–éŸ³", "å¿«æ‰‹"], "users": ["weicungang"]}  # æµ‹è¯•æœŸé—´ç»Ÿä¸€å‘é€ç»™ weicungang
}

# å›ºå®šåˆ—å
DATE_COL = 'äº¤æ˜“æ—¶é—´'
AMOUNT_COL = 'åˆ†æ‘Šåæ€»ä»·'
QTY_COL = 'å®å‘æ•°é‡'
SHOP_COL = 'åº—é“º'
CATEGORY_COL = 'è´§å“åç§°'
MODEL_COL = 'è§„æ ¼åç§°'

# ========== å·¥å…·å‡½æ•° ==========
def check_required_columns(df):
    required_cols = [DATE_COL, AMOUNT_COL, QTY_COL, SHOP_COL, CATEGORY_COL, MODEL_COL]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„åˆ—: {missing}")
        print(f"å½“å‰åˆ—: {list(df.columns)}")
        sys.exit(1)

def check_and_fix_column_names(df):
    """æ£€æŸ¥å’Œä¿®æ­£åˆ—å"""
    print(f"ğŸ” æ£€æŸ¥åˆ—åï¼Œå½“å‰åˆ—: {list(df.columns)}")
    
    # å®šä¹‰å¯èƒ½çš„åˆ—åæ˜ å°„ï¼ˆæ‰©å±•ç‰ˆï¼‰
    column_mappings = {
        'åˆ†æ‘Šåæ€»ä»·': ['åˆ†æ‘Šåæ€»ä»·', 'æ€»ä»·', 'é‡‘é¢', 'é”€å”®é‡‘é¢', 'è®¢å•é‡‘é¢', 'å®ä»˜é‡‘é¢', 'æ”¯ä»˜é‡‘é¢', 'é€€æ¬¾å‰æ”¯ä»˜é‡‘é¢', 'åˆ†æ‘Šé‡‘é¢', 'åˆ†æ‘Šæ€»ä»·'],
        'å®å‘æ•°é‡': ['å®å‘æ•°é‡', 'æ•°é‡', 'é”€å”®æ•°é‡', 'è®¢å•æ•°é‡', 'å•†å“æ•°é‡', 'å‘è´§æ•°é‡', 'å®é™…æ•°é‡'],
        'åº—é“º': ['åº—é“º', 'åº—é“ºåç§°', 'åº—é“ºå', 'é”€å”®åº—é“º', 'æ¸ é“åº—é“º', 'é—¨åº—', 'å•†åº—'],
        'è´§å“åç§°': ['è´§å“åç§°', 'å•†å“åç§°', 'äº§å“åç§°', 'å“ç±»', 'å•†å“å“ç±»', 'äº§å“å“ç±»', 'å•†å“', 'äº§å“'],
        'è§„æ ¼åç§°': ['è§„æ ¼åç§°', 'å‹å·', 'å•†å“å‹å·', 'äº§å“å‹å·', 'è§„æ ¼', 'äº§å“è§„æ ¼', 'å•†å“è§„æ ¼'],
        'äº¤æ˜“æ—¶é—´': ['äº¤æ˜“æ—¶é—´', 'ä¸‹å•æ—¶é—´', 'è®¢å•æ—¶é—´', 'åˆ›å»ºæ—¶é—´', 'æ—¶é—´', 'æ—¥æœŸ', 'äº¤æ˜“æ—¥æœŸ', 'è®¢å•æ—¥æœŸ'],
        'å®¢æœå¤‡æ³¨': ['å®¢æœå¤‡æ³¨', 'å®¢æœå¤‡æ³¨', 'å–å®¶å¤‡æ³¨', 'å•†å®¶å¤‡æ³¨', 'è®¢å•å¤‡æ³¨'],
        'ä¹°å®¶ç•™è¨€': ['ä¹°å®¶ç•™è¨€', 'ä¹°å®¶å¤‡æ³¨', 'å®¢æˆ·ç•™è¨€', 'ç•™è¨€', 'ä¹°å®¶æ¶ˆæ¯'],
        'å¤‡æ³¨': ['å¤‡æ³¨', 'è®¢å•å¤‡æ³¨', 'ç‰¹æ®Šå¤‡æ³¨', 'è¯´æ˜']
    }
    
    # æ£€æŸ¥å¹¶ä¿®æ­£åˆ—å
    fixed_columns = {}
    for target_col, possible_names in column_mappings.items():
        if target_col not in df.columns:
            found = False
            for possible_name in possible_names:
                if possible_name in df.columns:
                    print(f"   âœ… æ‰¾åˆ°åˆ—åæ˜ å°„: {possible_name} -> {target_col}")
                    fixed_columns[possible_name] = target_col
                    found = True
                    break
            if not found:
                print(f"   âŒ æœªæ‰¾åˆ°åˆ—å: {target_col}")
                print(f"      å¯èƒ½çš„åˆ—å: {possible_names}")
                print(f"      å®é™…åˆ—å: {list(df.columns)}")
        else:
            print(f"   âœ… åˆ—åå·²å­˜åœ¨: {target_col}")
    
    # æ‰§è¡Œåˆ—åé‡å‘½å
    if fixed_columns:
        df = df.rename(columns=fixed_columns)
        print(f"   ğŸ”„ å·²é‡å‘½ååˆ—: {fixed_columns}")
    
    return df

def to_number(val):
    if pd.isnull(val):
        return 0
    val = str(val).replace('ï¼Œ', '').replace(',', '').replace(' ', '').replace('\u3000', '')
    # å¤„ç†å¯èƒ½çš„ç§‘å­¦è®¡æ•°æ³•æˆ–å…¶ä»–æ ¼å¼
    try:
        # å…ˆå°è¯•ç›´æ¥è½¬æ¢
        return int(float(val))  # ç›´æ¥è¿”å›æ•´æ•°ï¼Œé¿å…å°æ•°ä½
    except ValueError:
        try:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•æå–æ•°å­—éƒ¨åˆ†
            import re
            numbers = re.findall(r'[\d.]+', val)
            if numbers:
                # å¦‚æœæ‰¾åˆ°å¤šä¸ªæ•°å­—ï¼Œå–ç¬¬ä¸€ä¸ª
                return int(float(numbers[0]))  # ç›´æ¥è¿”å›æ•´æ•°
            else:
                return 0
        except:
            return 0

def is_online_shop(shop_name):
    if not isinstance(shop_name, str):
        return False
    online_keywords = ['äº¬ä¸œ','å¤©çŒ«','æ‹¼å¤šå¤š','æŠ–éŸ³','å¡è¨å¸','å°çº¢ä¹¦','æ·˜å®','è‹å®','å›½ç¾']
    return any(kw in shop_name for kw in online_keywords)

def get_target_users(group_name, group_type):
    """æ ¹æ®åˆ†ç»„åç§°å’Œç±»å‹è·å–ç›®æ ‡ç”¨æˆ·ï¼Œç¡®ä¿å»é‡"""
    users = set(always_users)  # å›ºå®šæ”¶ä»¶äºº
    
    if group_type == 'business':
        # äº‹ä¸šéƒ¨åˆ†ç»„ï¼Œæ ¹æ®å…³é”®è¯åŒ¹é…ç”¨æˆ·
        for dept, conf in business_groups.items():
            if dept == group_name:
                users.update(conf["users"])
                break
    else:
        # æ¸ é“åˆ†ç»„ï¼Œæ ¹æ®æ¸ é“åŒ¹é…ç”¨æˆ·
        for channel, conf in CHANNEL_GROUPS.items():
            if channel == group_name:
                users.update(conf["users"])
                break
    
    # ç¡®ä¿ç”¨æˆ·åˆ—è¡¨å»é‡
    target_users = list(set(users))
    return target_users

def _send_single_message(msg, target_user=None):
    """å‘é€å•æ¡æ¶ˆæ¯ï¼Œæ”¯æŒ5æ¬¡é‡è¯•å’Œå¤±è´¥webhooké€šçŸ¥"""
    to_user = target_user if target_user else WECHAT_USER
    data = {"msg": msg, "token": WECHAT_TOKEN, "to_user": to_user}
    
    max_retries = 5
    retry_delay = 3
    
    for attempt in range(max_retries):
        try:
            logger.info(f"ğŸ“¤ å°è¯•å‘é€æ¶ˆæ¯ç»™ç”¨æˆ· {to_user} (ç¬¬{attempt + 1}/{max_retries}æ¬¡)")
            resp = requests.post(WECHAT_API, json=data, timeout=15)
            logger.info(f"ğŸ“¤ å‘é€ç»“æœ: {resp.text[:100]}...")
            
            if "errcode" in resp.text and "0" in resp.text:
                logger.info(f"âœ… å‘é€æˆåŠŸ (å°è¯• {attempt + 1}/{max_retries})")
                return True
            elif "500" in resp.text or "error" in resp.text.lower():
                logger.warning(f"âš ï¸ æœåŠ¡å™¨é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    logger.info(f"â³ {retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5
                    # å°è¯•ç¼©çŸ­å†…å®¹é‡è¯•
                    shorter_msg = msg[:500]
                    data["msg"] = shorter_msg
                else:
                    logger.error(f"âŒ å‘é€å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                    # å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook
                    send_failure_webhook_notification(to_user, msg, f"æœåŠ¡å™¨é”™è¯¯: {resp.text}")
                    return False
            else:
                logger.warning(f"âš ï¸ å‘é€è¿”å›å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {resp.text}")
                if attempt < max_retries - 1:
                    logger.info(f"â³ {retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    retry_delay *= 1.5
                else:
                    logger.error(f"âŒ å‘é€å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                    # å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook
                    send_failure_webhook_notification(to_user, msg, f"å‘é€å¼‚å¸¸: {resp.text}")
                    return False
        except requests.exceptions.ConnectTimeout:
            logger.error(f"âŒ è¿æ¥è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                logger.info(f"â³ {retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 1.5
            else:
                logger.error(f"âŒ å‘é€å¤±è´¥: è¿æ¥è¶…æ—¶ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                # å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook
                send_failure_webhook_notification(to_user, msg, "è¿æ¥è¶…æ—¶")
                return False
        except requests.exceptions.Timeout:
            logger.error(f"âŒ è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                logger.info(f"â³ {retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 1.5
            else:
                logger.error(f"âŒ å‘é€å¤±è´¥: è¯·æ±‚è¶…æ—¶ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                # å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook
                send_failure_webhook_notification(to_user, msg, "è¯·æ±‚è¶…æ—¶")
                return False
        except Exception as e:
            logger.error(f"âŒ å‘é€å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                logger.info(f"â³ {retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
                retry_delay *= 1.5
            else:
                logger.error(f"âŒ å‘é€å¤±è´¥: {e}ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                # å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook
                send_failure_webhook_notification(to_user, msg, f"å‘é€å¼‚å¸¸: {str(e)}")
                return False
    return False

def send_failure_webhook_notification(target_user, message_content, error_details):
    """å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook"""
    webhook_url = "https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=02d1441f-aa5b-44cb-aeab-b934fe78f8cb"
    
    failure_msg = f"""ğŸš¨ å¤šäº‹ä¸šéƒ¨æ—¥æŠ¥æ•°æ®å‘é€å¤±è´¥é€šçŸ¥

ğŸ“‹ å¤±è´¥è¯¦æƒ…:
â€¢ ç›®æ ‡ç”¨æˆ·: {target_user}
â€¢ å¤±è´¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
â€¢ é”™è¯¯åŸå› : {error_details}
â€¢ æ¶ˆæ¯é•¿åº¦: {len(message_content)} å­—ç¬¦

ğŸ“ æ¶ˆæ¯å†…å®¹é¢„è§ˆ:
{message_content[:200]}{'...' if len(message_content) > 200 else ''}

è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡å™¨çŠ¶æ€ã€‚"""
    
    try:
        webhook_data = {
            "msgtype": "text",
            "text": {
                "content": failure_msg
            }
        }
        
        logger.info(f"ğŸ“¤ å‘é€å¤±è´¥é€šçŸ¥åˆ°webhook: {webhook_url}")
        response = requests.post(webhook_url, json=webhook_data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('errcode') == 0:
                logger.info("âœ… å¤±è´¥é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                logger.error(f"âŒ å¤±è´¥é€šçŸ¥å‘é€å¤±è´¥: {result}")
        else:
            logger.error(f"âŒ å¤±è´¥é€šçŸ¥HTTPé”™è¯¯: {response.status_code}")
            
    except Exception as e:
        logger.error(f"âŒ å‘é€å¤±è´¥é€šçŸ¥å¼‚å¸¸: {e}")

def send_wecomchan_segment(content, target_users=None):
    """åˆ†æ®µå‘é€ï¼Œç¡®ä¿é“¾æ¥ä¼˜å…ˆå‘é€ï¼Œæ”¯æŒå¤šç”¨æˆ·å‘é€å’Œå»é‡"""
    max_chars = 1500  # ä¿®æ­£å­—ç¬¦é™åˆ¶ä¸º1500
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ç”¨æˆ·ï¼Œä½¿ç”¨é»˜è®¤ç”¨æˆ·
    if target_users is None:
        target_users = [WECHAT_USER]
    
    # ç¡®ä¿ç”¨æˆ·åˆ—è¡¨å»é‡
    target_users = list(set(target_users))
    
    logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€ç»™ {len(target_users)} ä¸ªç”¨æˆ·: {', '.join(target_users)}")
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·å‘é€æ¶ˆæ¯
    success_count = 0
    for user in target_users:
        logger.info(f"ğŸ“¤ æ­£åœ¨å‘é€ç»™ç”¨æˆ·: {user}")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥
        link_pattern = r'ğŸŒ æŸ¥çœ‹å®Œæ•´Webé¡µé¢: (https://[^\s]+)'
        link_match = re.search(link_pattern, content)
        
        if link_match:
            link = link_match.group(1)
            link_text = f"ğŸŒ æŸ¥çœ‹å®Œæ•´Webé¡µé¢: {link}"
            content_without_link = content.replace(link_text, "").strip()
            
            # å¦‚æœå†…å®¹é•¿åº¦è¶…è¿‡é™åˆ¶ï¼Œä¼˜å…ˆä¿è¯é“¾æ¥å‘é€
            if len(content_without_link) + len(link_text) > max_chars:
                # æˆªæ–­ä¸»å†…å®¹ï¼Œä¿ç•™é“¾æ¥
                available_chars = max_chars - len(link_text) - 10  # ç•™ä¸€äº›ç¼“å†²
                truncated_content = content_without_link[:available_chars].rstrip('\n')
                final_content = truncated_content + f"\n{link_text}"
                logger.warning(f"âš ï¸ å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {len(final_content)} å­—ç¬¦")
            else:
                final_content = content
        else:
            # æ²¡æœ‰é“¾æ¥ï¼Œç›´æ¥æˆªæ–­
            if len(content) > max_chars:
                final_content = content[:max_chars].rstrip('\n')
                logger.warning(f"âš ï¸ å†…å®¹è¿‡é•¿ä¸”æ— é“¾æ¥ï¼Œå·²æˆªæ–­è‡³ {len(final_content)} å­—ç¬¦")
            else:
                final_content = content
        
        # å‘é€æ¶ˆæ¯
        success = _send_single_message(final_content, user)
        if success:
            success_count += 1
            logger.info(f"âœ… å‘é€æˆåŠŸç»™ç”¨æˆ·: {user}")
        else:
            logger.error(f"âŒ å‘é€å¤±è´¥ç»™ç”¨æˆ·: {user}")
        
        # ç”¨æˆ·é—´å‘é€é—´éš”
        time.sleep(1)
    
    logger.info(f"ğŸ“Š å‘é€å®Œæˆ: {success_count}/{len(target_users)} æˆåŠŸ")
    return success_count == len(target_users)

def _smart_split_content(content, max_chars):
    """æ™ºèƒ½åˆ†å‰²å†…å®¹"""
    # æŒ‰ç…§è‡ªç„¶åˆ†æ®µç¬¦åˆ†å‰²
    natural_breaks = ['\n\n', '\nâ”â”â”', '\n===', '\n---', '\nğŸ“Š', '\nğŸ”¥', '\nğŸ’°']
    
    segments = []
    current_segment = ""
    
    lines = content.split('\n')
    for line in lines:
        if len(current_segment + line + '\n') > max_chars:
            if current_segment:
                segments.append(current_segment.strip())
                current_segment = line + '\n'
            else:
                # å•è¡Œå¤ªé•¿ï¼Œå¼ºåˆ¶æˆªæ–­
                segments.append(line[:max_chars])
                current_segment = ""
        else:
            current_segment += line + '\n'
    
    if current_segment.strip():
        segments.append(current_segment.strip())
    
    return segments

def calculate_ratio(current, previous):
    """è®¡ç®—ç¯æ¯”ï¼Œè¿”å›æ•´æ•°ç™¾åˆ†æ¯”ï¼Œæ— å°æ•°ç‚¹"""
    if previous == 0:
        return "+100%" if current > 0 else "0%"
    ratio = int(((current - previous) / previous) * 100)
    if ratio > 0:
        return f"+{ratio}%"
    elif ratio < 0:
        return f"{ratio}%"
    else:
        return "0%"

def to_pinyin(s):
    mapping = {
        'ç©ºè°ƒäº‹ä¸šéƒ¨': 'kongtiaoshiyebu',
        'åˆ¶å†·äº‹ä¸šéƒ¨': 'zhilingshiyebu',
        'æ´—æŠ¤äº‹ä¸šéƒ¨': 'xihushiyebu',
        'æ°´è”ç½‘äº‹ä¸šéƒ¨': 'shuilianwangshiyebu',
        'å¨ç”µæ´—ç¢—æœºäº‹ä¸šéƒ¨': 'chudianxiwangjishiyebu',
        'å¡è¨å¸æ¸ é“': 'kasadiqudao',
        'å¤©çŒ«æ¸ é“': 'tianmaoqudao',
        'äº¬ä¸œæ¸ é“': 'jingdongqudao',
        'æ‹¼å¤šå¤šæ¸ é“': 'pinduoduoqudao',
        'æŠ–éŸ³æ¸ é“': 'douyinqudao'
    }
    if s in mapping:
        return mapping[s]
    s_ascii = unicodedata.normalize('NFKD', s).encode('ascii', 'ignore').decode('ascii')
    s_ascii = re.sub(r'[^a-zA-Z0-9_]', '', s_ascii)
    return s_ascii.lower() or 'report'

def generate_html_content(title_cn, content_text):
    """ç»Ÿä¸€çš„HTMLç”Ÿæˆå‡½æ•°ï¼Œç¡®ä¿æ‰€æœ‰æŠ¥å‘Šä½¿ç”¨ç›¸åŒçš„æ ¼å¼"""
    html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
    <title>{title_cn}</title>
    <style>
        body {{ font-family: 'Microsoft YaHei', 'å¾®è½¯é›…é»‘', Arial, sans-serif; background: #f8f9fa; color: #222; margin: 0; padding: 0.5em; max-width: 900px; margin-left:auto; margin-right:auto; text-align: left; font-size: 10.5pt; }}
        h1, h2, h3 {{ color: #0056b3; text-align: left; font-family: 'Microsoft YaHei', 'å¾®è½¯é›…é»‘', Arial, sans-serif; font-size: 14pt; font-weight: bold; margin: 0.3em 0; }}
        pre, code {{ background: #f3f3f3; padding: 0.5em; border-radius: 4px; white-space: pre-wrap; word-break: break-all; text-align: left; font-family: 'Microsoft YaHei', 'å¾®è½¯é›…é»‘', Arial, sans-serif; margin: 0.3em 0; }}
        .growth-positive {{ background-color: #e6f4ea !important; }}
        .growth-negative {{ background-color: #fbeaea !important; }}
        .section {{ margin-bottom: 2em; text-align: left; }}
        .highlight {{ color: #d63384; font-weight: bold; }}
        .emoji {{ font-size: 1.2em; }}
        @media (max-width: 600px) {{
            body {{ padding: 0.5em; font-size: 10.5pt; }}
            h1 {{ font-size: 14pt; }}
        }}
        .left-align {{ text-align: left !important; }}
    </style>
</head>
<body>
    <h1>{title_cn}</h1>
    <div class="section left-align">
        <pre>
{content_text}
        </pre>
    </div>
    <footer style="margin-top:2em;color:#888;font-size:0.9em;">è‡ªåŠ¨ç”Ÿæˆ | Powered by EdgeOne Pages & ä¼ä¸šå¾®ä¿¡æœºå™¨äºº</footer>
</body>
</html>'''
    return html_content

# ========== æ•°æ®è¯»å–ä¸é¢„å¤„ç† ==========
# æ–°å¢å‡½æ•°ï¼šä»æ•°æ®åº“è·å–åˆ†é”€æ•°æ®
def get_fenxiao_data(report_date):
    """ä»HT_fenxiaoè¡¨è·å–åˆ†é”€æ•°æ®"""
    try:
        conn = pymysql.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, 
            password=DB_PASSWORD, database=DB_NAME, charset=DB_CHARSET,
            connect_timeout=10
        )
        
        # ä½¿ç”¨fenxiaochanpinè¡¨è¿›è¡Œå•†å“åŒ¹é…ï¼ˆå·²ç¡®è®¤å­˜åœ¨æ•°æ®ï¼‰
        cursor = conn.cursor()
        
        # è·å–HT_fenxiaoè¡¨ç»“æ„
        cursor.execute("DESCRIBE HT_fenxiao")
        columns = [row[0] for row in cursor.fetchall()]
        logger.info(f"ğŸ“Š HT_fenxiaoè¡¨å­—æ®µ: {columns}")
        
        # æŸ¥æ‰¾å¯èƒ½çš„å­—æ®µå
        amount_fields = [col for col in columns if 'é‡‘é¢' in col or 'å®ä»˜' in col or 'æ”¯ä»˜' in col]
        shop_fields = [col for col in columns if 'åº—é“º' in col or 'å•†åº—' in col]
        status_fields = [col for col in columns if 'çŠ¶æ€' in col or 'è®¢å•' in col]
        time_fields = [col for col in columns if 'æ—¶é—´' in col or 'æ”¯ä»˜' in col]
        product_fields = [col for col in columns if 'äº§å“' in col or 'åç§°' in col]
        qty_fields = [col for col in columns if 'æ•°é‡' in col or 'é‡‡è´­æ•°é‡' in col]
        
        # é€‰æ‹©æœ€åˆé€‚çš„å­—æ®µå
        amount_col = 'ç”¨æˆ·å®é™…æ”¯ä»˜æ€»é¢' if 'ç”¨æˆ·å®é™…æ”¯ä»˜æ€»é¢' in columns else (amount_fields[0] if amount_fields else 'ç”¨æˆ·å®é™…æ”¯ä»˜é‡‘é¢')
        shop_col = 'åˆ†é”€å•†åº—é“ºåç§°' if 'åˆ†é”€å•†åº—é“ºåç§°' in columns else (shop_fields[0] if shop_fields else 'åˆ†é”€å•†åº—é“ºåç§°')
        status_col = 'è®¢å•çŠ¶æ€' if 'è®¢å•çŠ¶æ€' in columns else (status_fields[0] if status_fields else 'è®¢å•çŠ¶æ€')
        time_col = 'é‡‡è´­å•æ”¯ä»˜æ—¶é—´' if 'é‡‡è´­å•æ”¯ä»˜æ—¶é—´' in columns else (time_fields[0] if time_fields else 'é‡‡è´­å•æ”¯ä»˜æ—¶é—´')
        product_col = 'äº§å“åç§°' if 'äº§å“åç§°' in columns else (product_fields[0] if product_fields else 'äº§å“åç§°')
        qty_col = 'é‡‡è´­æ•°é‡' if 'é‡‡è´­æ•°é‡' in columns else (qty_fields[0] if qty_fields else 'é‡‡è´­æ•°é‡')
        
        # æŸ¥è¯¢åˆ†é”€æ•°æ®ï¼Œä½¿ç”¨åŠ¨æ€å­—æ®µåï¼Œç¡®ä¿è®¢å•çŠ¶æ€è¿‡æ»¤ç”Ÿæ•ˆ
        # åªè¿‡æ»¤æ‰ï¼šæœªä»˜æ¬¾ã€å·²å–æ¶ˆã€å·²é€€è´§
        sql = f"""
        SELECT 
            {shop_col} as åº—é“º,
            {status_col} as è®¢å•çŠ¶æ€,
            {amount_col} as åˆ†æ‘Šåæ€»ä»·,
            {time_col} as äº¤æ˜“æ—¶é—´,
            {product_col} as è§„æ ¼åç§°,
            {product_col} as è´§å“åç§°,
            {qty_col} as å®å‘æ•°é‡,
            'åˆ†é”€' as æ•°æ®æ¥æº
        FROM HT_fenxiao 
        WHERE {time_col} LIKE '{report_date}%'
        AND {status_col} NOT IN ('å·²å–æ¶ˆ', 'æœªä»˜æ¬¾', 'å·²é€€è´§')
        """
        
        logger.info(f"ğŸ“Š æ‰§è¡ŒSQL: {sql}")
        
        df_fenxiao = pd.read_sql(sql, conn)
        
        if not df_fenxiao.empty:
            logger.info(f"ğŸ“Š åˆ†é”€æ•°æ®è·å–æˆåŠŸï¼Œå…±{len(df_fenxiao)}è¡Œï¼ˆå·²è¿‡æ»¤æ— æ•ˆè®¢å•çŠ¶æ€ï¼‰")
            
            # æ˜¾ç¤ºè®¢å•çŠ¶æ€åˆ†å¸ƒï¼Œç¡®è®¤è¿‡æ»¤ç”Ÿæ•ˆ
            status_counts = df_fenxiao['è®¢å•çŠ¶æ€'].value_counts()
            logger.info(f"ğŸ“Š è¿‡æ»¤åè®¢å•çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in status_counts.items():
                logger.info(f"   {status}: {count}æ¡")
            
            # ä¼˜åŒ–äº§å“åŒ¹é…é€»è¾‘ï¼šæ‰¹é‡å¤„ç†é¿å…é‡å¤æŸ¥è¯¢
            logger.info("ğŸ”„ å°è¯•ä»fenxiaochanpinè¡¨åŒ¹é…è§„æ ¼åç§°å’Œè´§å“åç§°...")
            
            # è·å–æ‰€æœ‰å”¯ä¸€çš„äº§å“åç§°
            unique_products = df_fenxiao['è§„æ ¼åç§°'].dropna().unique()
            logger.info(f"ğŸ“Š éœ€è¦åŒ¹é…çš„å”¯ä¸€äº§å“æ•°é‡: {len(unique_products)}")
            
            # æ„å»ºæœ€ç»ˆçš„äº§å“æ˜ å°„ç¼“å­˜
            final_product_mapping = {}
            
            # æ‰¹é‡æŸ¥è¯¢fenxiaochanpinè¡¨
            if len(unique_products) > 0:
                placeholders = ','.join(['%s'] * len(unique_products))
                batch_sql = f"SELECT äº§å“åç§°, è§„æ ¼åç§°, è´§å“åç§° FROM fenxiaochanpin WHERE äº§å“åç§° IN ({placeholders})"
                cursor.execute(batch_sql, tuple(unique_products))
                fenxiao_results = cursor.fetchall()
                
                # æ„å»ºfenxiaochanpinæ˜ å°„
                fenxiao_mapping = {}
                for product_name, model_name, goods_name in fenxiao_results:
                    fenxiao_mapping[product_name] = (model_name, goods_name)
                
                logger.info(f"ğŸ“Š fenxiaochanpinè¡¨åŒ¹é…åˆ° {len(fenxiao_mapping)} ä¸ªäº§å“")
                
                # å¤„ç†æ¯ä¸ªå”¯ä¸€äº§å“
                for product_name in unique_products:
                    if not isinstance(product_name, str):
                        continue
                        
                    if product_name in fenxiao_mapping:
                        matched_model_name, matched_product_name = fenxiao_mapping[product_name]
                        
                        # å¦‚æœè´§å“åç§°ä¸ºç©ºï¼Œä»ERPæ•°æ®æŸ¥è¯¢
                        if matched_product_name is None or matched_product_name == '':
                            erp_sql = "SELECT è´§å“åç§° FROM Daysales WHERE è§„æ ¼åç§° = %s AND è´§å“åç§° IS NOT NULL AND è´§å“åç§° != '' LIMIT 1"
                            cursor.execute(erp_sql, (matched_model_name,))
                            erp_result = cursor.fetchone()
                            if erp_result:
                                matched_product_name = erp_result[0]
                                logger.info(f"   âœ… ä»ERPæ•°æ®åŒ¹é…è´§å“åç§°: {matched_model_name} -> {matched_product_name}")
                            else:
                                # å…³é”®è¯åŒ¹é…
                                import re
                                keywords = []
                                model_patterns = re.findall(r'[A-Z0-9]{3,}', product_name.upper())
                                keywords.extend(model_patterns)
                                chinese_words = re.findall(r'[\u4e00-\u9fff]+', product_name)
                                for word in chinese_words:
                                    if len(word) >= 2 and word not in ['å¤§å®¹é‡', 'å®¶ç”¨', 'ç‹¬ç«‹å¼', 'å˜é¢‘', 'è¶…ä¸€çº§', 'æ°´æ•ˆ', 'å‡çº§æ¬¾', 'å°±è¿‘ä»“', 'æ–°å“ä¸Šå¸‚', 'è¶…å¤§å®¹é‡']:
                                        keywords.append(word)
                                
                                matched_by_keyword = False
                                for keyword in keywords[:3]:
                                    keyword_sql = "SELECT è´§å“åç§° FROM Daysales WHERE è´§å“åç§° LIKE %s AND è´§å“åç§° IS NOT NULL AND è´§å“åç§° != '' LIMIT 1"
                                    cursor.execute(keyword_sql, (f'%{keyword}%',))
                                    keyword_result = cursor.fetchone()
                                    if keyword_result:
                                        matched_product_name = keyword_result[0]
                                        logger.info(f"   âœ… é€šè¿‡å…³é”®è¯'{keyword}'åŒ¹é…åˆ°è´§å“åç§°: {matched_product_name}")
                                        matched_by_keyword = True
                                        break
                                
                                if not matched_by_keyword:
                                    matched_product_name = force_categorize_product(product_name)
                                    logger.info(f"   âš ï¸ æ‰€æœ‰åŒ¹é…æ–¹å¼éƒ½å¤±è´¥ï¼Œå¼ºåˆ¶åŒ¹é…åˆ°å“ç±»: {product_name} -> {matched_product_name}")
                        
                        final_product_mapping[product_name] = {
                            'è§„æ ¼åç§°': matched_model_name,
                            'è´§å“åç§°': matched_product_name
                        }
                        logger.info(f"   âœ… åŒ¹é…æˆåŠŸ: {product_name} -> è§„æ ¼åç§°:{matched_model_name}, è´§å“åç§°:{matched_product_name}")
                    else:
                        logger.info(f"   âš ï¸ æœªåŒ¹é…åˆ°: {product_name}ï¼Œä½¿ç”¨åŸäº§å“åç§°")
                        final_product_mapping[product_name] = {
                            'è§„æ ¼åç§°': product_name,
                            'è´§å“åç§°': product_name
                        }
            
            # æ‰¹é‡åº”ç”¨æ˜ å°„ç»“æœ
            logger.info("ğŸ”„ æ‰¹é‡åº”ç”¨äº§å“æ˜ å°„ç»“æœ...")
            for index, row in df_fenxiao.iterrows():
                product_name = row['è§„æ ¼åç§°']
                if isinstance(product_name, str) and product_name in final_product_mapping:
                    mapping = final_product_mapping[product_name]
                    df_fenxiao.at[index, 'è§„æ ¼åç§°'] = mapping['è§„æ ¼åç§°']
                    df_fenxiao.at[index, 'è´§å“åç§°'] = mapping['è´§å“åç§°']
            
            # æ·»åŠ å“ç±»å­—æ®µ
            logger.info("ğŸ”„ æ·»åŠ å“ç±»å­—æ®µ...")
            df_fenxiao['å“ç±»'] = df_fenxiao['è´§å“åç§°'].apply(categorize_product)
            
            # ä¸ºæ‰€æœ‰ht_fenxiaoæ•°æ®ç»Ÿä¸€æ·»åŠ "äº¬ä¸œ-"å‰ç¼€
            def add_jingdong_prefix(shop_name):
                if not isinstance(shop_name, str):
                    return shop_name
                
                # å¦‚æœå·²ç»æœ‰"äº¬ä¸œ-"å‰ç¼€ï¼Œç›´æ¥è¿”å›
                if shop_name.startswith('äº¬ä¸œ-'):
                    return shop_name
                
                # ç»Ÿä¸€æ·»åŠ "äº¬ä¸œ-"å‰ç¼€
                return f"äº¬ä¸œ-{shop_name}"
            
            # ä¸ºæ‰€æœ‰åˆ†é”€å•†åº—é“ºåç§°æ·»åŠ "äº¬ä¸œ-"å‰ç¼€
            df_fenxiao['åº—é“º'] = df_fenxiao['åº—é“º'].apply(add_jingdong_prefix)
            
            logger.info(f"ğŸ“Š åˆ†é”€æ•°æ®å­—æ®µ: {df_fenxiao.columns.tolist()}")
            logger.info(f"ğŸ“Š åˆ†é”€æ•°æ®å‰3è¡Œ:")
            for i, row in df_fenxiao.head(3).iterrows():
                logger.info(f"   è¡Œ{i+1}: {dict(row)}")
            
            conn.close()
            return df_fenxiao
        else:
            logger.info("ğŸ“Š æœªè·å–åˆ°åˆ†é”€æ•°æ®")
            conn.close()
            return pd.DataFrame()
            
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ†é”€æ•°æ®å¤±è´¥: {e}")
        if 'conn' in locals():
            conn.close()
        return pd.DataFrame()

def get_product_category_from_db(product_name):
    """ä»fenxiaochanpinæ•°æ®åº“è¡¨ä¸­è·å–äº§å“å“ç±»ä¿¡æ¯"""
    if not isinstance(product_name, str):
        return None
    
    try:
        conn = pymysql.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, 
            password=DB_PASSWORD, database=DB_NAME, charset=DB_CHARSET,
            connect_timeout=10
        )
        
        # é¦–å…ˆæ£€æŸ¥fenxiaochanpinè¡¨æ˜¯å¦å­˜åœ¨
        cursor = conn.cursor()
        cursor.execute("SHOW TABLES LIKE 'fenxiaochanpin'")
        if not cursor.fetchone():
            logger.warning("âš ï¸ fenxiaochanpinè¡¨ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨äº§å“åç§°è¯†åˆ«å“ç±»")
            conn.close()
            return None
        
        # æ£€æŸ¥è¡¨ç»“æ„ï¼ŒæŸ¥æ‰¾å“ç±»å­—æ®µ
        cursor.execute("DESCRIBE fenxiaochanpin")
        columns = [row[0] for row in cursor.fetchall()]
        logger.info(f"ğŸ“Š fenxiaochanpinè¡¨å­—æ®µ: {columns}")
        
        # æŸ¥æ‰¾äº§å“åç§°å’Œå“ç±»å­—æ®µ
        product_cols = [col for col in columns if 'äº§å“' in col or 'åç§°' in col or 'è§„æ ¼' in col]
        category_cols = [col for col in columns if 'å“ç±»' in col or 'åˆ†ç±»' in col or 'category' in col.lower()]
        
        if not product_cols or not category_cols:
            logger.warning("âš ï¸ fenxiaochanpinè¡¨ä¸­æœªæ‰¾åˆ°äº§å“åç§°æˆ–å“ç±»å­—æ®µ")
            conn.close()
            return None
        
        product_col = product_cols[0]
        category_col = category_cols[0]
        
        logger.info(f"ğŸ” ä½¿ç”¨å­—æ®µ: äº§å“åç§°={product_col}, å“ç±»={category_col}")
        
        # æŸ¥è¯¢äº§å“å“ç±»
        sql = f"SELECT {category_col} FROM fenxiaochanpin WHERE {product_col} = %s LIMIT 1"
        cursor.execute(sql, (product_name,))
        result = cursor.fetchone()
        
        if result and result[0]:
            category = result[0]
            logger.info(f"âœ… æ•°æ®åº“åŒ¹é…æˆåŠŸ: {product_name} -> {category}")
            conn.close()
            return category
        else:
            logger.info(f"âš ï¸ æ•°æ®åº“æœªåŒ¹é…åˆ°: {product_name}")
            conn.close()
            return None
            
    except Exception as e:
        logger.error(f"âŒ ä»fenxiaochanpinè¡¨è·å–å“ç±»å¤±è´¥: {e}")
        return None

def categorize_product(product_name):
    """ä»äº§å“åç§°ä¸­è¯†åˆ«å“ç±»ï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“åŒ¹é…"""
    if not isinstance(product_name, str):
        return "å…¶ä»–"
    
    # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–å“ç±»
    db_category = get_product_category_from_db(product_name)
    if db_category:
        return db_category
    
    # å¦‚æœæ•°æ®åº“åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯è¯†åˆ«
    product_name_lower = product_name.lower()
    
    # å“ç±»å…³é”®è¯æ˜ å°„
    category_keywords = {
        "å®¶ç”¨ç©ºè°ƒ": ["ç©ºè°ƒ", "æŒ‚æœº", "æŸœæœº", "ä¸­å¤®ç©ºè°ƒ", "åˆ†ä½“å¼"],
        "å•†ç”¨ç©ºè°ƒ": ["å•†ç”¨", "å•†ç”¨ç©ºè°ƒ", "å¤šè”æœº", "é£ç®¡æœº"],
        "å†°ç®±": ["å†°ç®±", "å†·æŸœ", "å†°æŸœ", "å†·è—", "å†·å†»"],
        "æ´—è¡£æœº": ["æ´—è¡£æœº", "æ´—çƒ˜ä¸€ä½“", "æ»šç­’", "æ³¢è½®"],
        "æ´—ç¢—æœº": ["æ´—ç¢—æœº", "æ´—ç¢—", "æ´—ç¢Ÿæœº"],  # æ´—ç¢—æœºç‹¬ç«‹ä¸ºä¸€ä¸ªå“ç±»
        "çƒ­æ°´å™¨": ["çƒ­æ°´å™¨", "ç”µçƒ­æ°´å™¨", "ç‡ƒæ°”çƒ­æ°´å™¨", "å¤šèƒ½æºçƒ­æ°´å™¨"],
        "å‡€æ°´": ["å‡€æ°´", "å‡€æ°´å™¨", "å‡€æ°´æœº", "è¿‡æ»¤å™¨"],
        "é‡‡æš–": ["é‡‡æš–", "æš–æ°”", "åœ°æš–", "å£æŒ‚ç‚‰"],
        "å¨ç”µ": ["å¨ç”µ", "æ²¹çƒŸæœº", "ç‡ƒæ°”ç¶", "æ¶ˆæ¯’æŸœ", "è’¸ç®±", "çƒ¤ç®±"]  # ç§»é™¤æ´—ç¢—æœº
    }
    
    for category, keywords in category_keywords.items():
        if any(keyword in product_name_lower for keyword in keywords):
            logger.info(f"ğŸ” å…³é”®è¯åŒ¹é…: {product_name} -> {category}")
            return category
    
    logger.info(f"âš ï¸ æœªåŒ¹é…åˆ°å“ç±»: {product_name}ï¼Œå½’ç±»ä¸ºå…¶ä»–")
    return "å…¶ä»–"

def force_categorize_product(product_name):
    """å¼ºåˆ¶å°†äº§å“åç§°åŒ¹é…åˆ°å…«ä¸ªé¢„å®šä¹‰å“ç±»ä¹‹ä¸€"""
    if not isinstance(product_name, str):
        return "å†°ç®±"
    
    product_name_lower = product_name.lower()
    
    # å…«ä¸ªé¢„å®šä¹‰å“ç±»çš„å…³é”®è¯
    category_keywords = {
        "æ´—ç¢—æœº": ["æ´—ç¢—æœº", "æ´—ç¢—", "æ´—ç¢Ÿæœº", "æ´—ç¢Ÿ"],
        "å†°ç®±": ["å†°ç®±", "å†·è—", "å†·å†»", "ä¿é²œ"],
        "æ´—è¡£æœº": ["æ´—è¡£æœº", "æ´—çƒ˜ä¸€ä½“", "æ»šç­’", "æ³¢è½®", "æ´—è¡£"],
        "å†·æŸœ": ["å†·æŸœ", "å†°æŸœ", "å±•ç¤ºæŸœ", "å†·è—æŸœ", "å†·å†»æŸœ"],
        "å®¶ç”¨ç©ºè°ƒ": ["ç©ºè°ƒ", "æŒ‚æœº", "æŸœæœº", "åˆ†ä½“å¼", "å£æŒ‚", "ç«‹å¼"],
        "å•†ç©ºç©ºè°ƒ": ["å•†ç”¨", "å•†ç”¨ç©ºè°ƒ", "å¤šè”æœº", "é£ç®¡æœº", "ä¸­å¤®ç©ºè°ƒ", "å•†ç©º"],
        "å¨ç”µ": ["å¨ç”µ", "æ²¹çƒŸæœº", "ç‡ƒæ°”ç¶", "æ¶ˆæ¯’æŸœ", "è’¸ç®±", "çƒ¤ç®±", "ç¶å…·"],
        "çƒ­æ°´å™¨": ["çƒ­æ°´å™¨", "ç”µçƒ­æ°´å™¨", "ç‡ƒæ°”çƒ­æ°´å™¨", "å¤šèƒ½æºçƒ­æ°´å™¨", "çƒ­æ°´"]
    }
    
    # ä¼˜å…ˆåŒ¹é…
    for category, keywords in category_keywords.items():
        if any(keyword in product_name_lower for keyword in keywords):
            return category
    
    # æ™ºèƒ½åˆ¤æ–­é€»è¾‘
    if any(word in product_name_lower for word in ["æµ·å°”", "ç¾çš„", "æ ¼åŠ›", "tcl", "å®¹å£°"]):
        if any(word in product_name_lower for word in ["å‡", "l", "å®¹é‡"]):
            return "å†°ç®±"
        elif any(word in product_name_lower for word in ["kg", "å…¬æ–¤", "æ´—"]):
            return "æ´—è¡£æœº"
        elif any(word in product_name_lower for word in ["åŒ¹", "p", "åˆ¶å†·", "åˆ¶çƒ­"]):
            return "å®¶ç”¨ç©ºè°ƒ"
    
    # é»˜è®¤å…œåº•
    return "å†°ç®±"

def identify_tianmao_fenxiao(df):
    """ä»åŸæœ‰æ•°æ®ä¸­è¯†åˆ«å¤©çŒ«åˆ†é”€æ•°æ®ï¼ˆä»“åº“å­—æ®µåŒ…å«'èœé¸Ÿä»“è‡ªæµè½¬'ï¼‰"""
    try:
        # æŸ¥æ‰¾ä»“åº“ç›¸å…³å­—æ®µ
        warehouse_cols = [col for col in df.columns if 'ä»“åº“' in col or 'warehouse' in col.lower()]
        
        if not warehouse_cols:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»“åº“å­—æ®µï¼Œæ— æ³•è¯†åˆ«å¤©çŒ«åˆ†é”€æ•°æ®")
            logger.info(f"ğŸ“Š å¯ç”¨å­—æ®µ: {df.columns.tolist()}")
            return None
        
        warehouse_col = warehouse_cols[0]
        logger.info(f"ğŸ” ä½¿ç”¨ä»“åº“å­—æ®µ: {warehouse_col}")
        
        # æ˜¾ç¤ºä»“åº“å­—æ®µçš„å”¯ä¸€å€¼ï¼Œå¸®åŠ©è°ƒè¯•
        unique_warehouses = df[warehouse_col].dropna().unique()
        logger.info(f"ğŸ“Š ä»“åº“å­—æ®µå”¯ä¸€å€¼: {unique_warehouses[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
        
        # ç­›é€‰å¤©çŒ«æ¸ é“ä¸”ä»“åº“åŒ…å«èœé¸Ÿä»“ç›¸å…³å…³é”®è¯çš„æ•°æ®
        tianmao_mask = df[SHOP_COL].astype(str).str.contains('å¤©çŒ«|æ·˜å®', na=False)
        warehouse_mask = df[warehouse_col].astype(str).str.contains('èœé¸Ÿä»“|èœé¸Ÿ|åˆ†é”€|åˆ†é”€ä»“', na=False)
        
        logger.info(f"ğŸ“Š å¤©çŒ«æ¸ é“æ•°æ®: {tianmao_mask.sum()}è¡Œ")
        logger.info(f"ğŸ“Š èœé¸Ÿä»“è‡ªæµè½¬æ•°æ®: {warehouse_mask.sum()}è¡Œ")
        
        tianmao_fenxiao = df[tianmao_mask & warehouse_mask].copy()
        
        if not tianmao_fenxiao.empty:
            # æ·»åŠ åˆ†é”€æ ‡è¯†
            tianmao_fenxiao['æ•°æ®æ¥æº'] = 'åˆ†é”€'
            # ä½¿ç”¨åŸæœ‰çš„è´§å“åç§°ï¼Œè€Œä¸æ˜¯è§„æ ¼åç§°çš„å“ç±»
            tianmao_fenxiao['å“ç±»'] = tianmao_fenxiao[CATEGORY_COL].apply(categorize_product)
            logger.info(f"ğŸ“Š è¯†åˆ«åˆ°å¤©çŒ«åˆ†é”€æ•°æ®: {len(tianmao_fenxiao)}è¡Œ")
            logger.info(f"ğŸ“Š å¤©çŒ«åˆ†é”€æ•°æ®ç¤ºä¾‹:")
            for i, row in tianmao_fenxiao.head(3).iterrows():
                logger.info(f"   åº—é“º: {row[SHOP_COL]}, ä»“åº“: {row[warehouse_col]}, é‡‘é¢: {row[AMOUNT_COL]}, å“ç±»: {row.get('å“ç±»', 'N/A')}")
            return tianmao_fenxiao
        else:
            logger.info("ğŸ“Š æœªè¯†åˆ«åˆ°å¤©çŒ«åˆ†é”€æ•°æ®")
            return None
            
    except Exception as e:
        logger.error(f"âŒ è¯†åˆ«å¤©çŒ«åˆ†é”€æ•°æ®å¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return None

# æ–°å¢å‡½æ•°ï¼šä»æ•°æ®åº“è·å–ERPæ•°æ®
def get_erp_data(report_date):
    try:
        conn = pymysql.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, 
            password=DB_PASSWORD, database=DB_NAME, charset=DB_CHARSET,
            connect_timeout=10
        )
        df = pd.read_sql(f"SELECT * FROM Daysales WHERE äº¤æ˜“æ—¶é—´ LIKE '{report_date}%'", conn)
        conn.close()
        return df
    except Exception as e:
        logger.error(f"âŒ ä»æ•°æ®åº“è·å–ERPæ•°æ®å¤±è´¥: {e}")
        return None

# æ–°å¢å‡½æ•°ï¼šä»æ•°æ®åº“è·å–åŒæœŸæ•°æ®
def get_prev_data(prev_date):
    try:
        conn = pymysql.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, 
            password=DB_PASSWORD, database=DB_NAME, charset=DB_CHARSET,
            connect_timeout=10
        )
        df = pd.read_sql(f"SELECT * FROM Daysales WHERE äº¤æ˜“æ—¶é—´ LIKE '{prev_date}%'", conn)
        conn.close()
        return df
    except Exception as e:
        logger.error(f"âŒ ä»æ•°æ®åº“è·å–åŒæœŸæ•°æ®å¤±è´¥: {e}")
        return None

# æ–°å¢å‡½æ•°ï¼šæ•°æ®é¢„å¤„ç†
def preprocess_data(df, df_fenxiao=None, df_tianmao_fenxiao=None):
    """é¢„å¤„ç†æ•°æ®ï¼ŒåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€ç±»å‹è½¬æ¢ã€è¿‡æ»¤ç­‰"""
    try:
        logger.info(f"ğŸ” å¼€å§‹é¢„å¤„ç†æ•°æ®ï¼ŒåŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
        
        # ç«‹å³æ£€æŸ¥å’Œä¿®æ­£åˆ—å
        df = check_and_fix_column_names(df)
        
        # 1. æ•°æ®ç±»å‹è½¬æ¢
        df[AMOUNT_COL] = pd.to_numeric(df[AMOUNT_COL], errors='coerce').fillna(0)
        df[QTY_COL] = pd.to_numeric(df[QTY_COL], errors='coerce').fillna(0)
        
        # 2. è¿‡æ»¤åˆ·å•æ•°æ® - é‡‘é¢è¿‡æ»¤ + å®¢æœå¤‡æ³¨å…³é”®è¯è¿‡æ»¤
        initial_count = len(df)
        df = df[df[AMOUNT_COL] > 0]
        after_amount_filter = len(df)
        
        # æ·»åŠ å®¢æœå¤‡æ³¨å…³é”®è¯è¿‡æ»¤
        def is_brushing_order(row):
            """åˆ¤æ–­æ˜¯å¦ä¸ºåˆ·å•è®¢å•"""
            keywords = ['æŠ½çº¸', 'çº¸å·¾', 'åˆ·å•', 'æµ‹è¯•', 'è™šæ‹Ÿ']
            exact_match_keywords = ['å®Œå…¨=ä¸å‘è´§', 'ä¸å‘è´§']  # ç²¾ç¡®åŒ¹é…
            
            # æ£€æŸ¥å®¢æœå¤‡æ³¨
            if 'å®¢æœå¤‡æ³¨' in row.index and pd.notna(row['å®¢æœå¤‡æ³¨']):
                remark = str(row['å®¢æœå¤‡æ³¨']).strip()
                # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
                for exact_keyword in exact_match_keywords:
                    if exact_keyword == remark:
                        return True
                # æ£€æŸ¥åŒ…å«å…³é”®è¯
                remark_lower = remark.lower()
                for keyword in keywords:
                    if keyword.lower() in remark_lower:
                        return True
            
            # æ£€æŸ¥ä¹°å®¶ç•™è¨€
            if 'ä¹°å®¶ç•™è¨€' in row.index and pd.notna(row['ä¹°å®¶ç•™è¨€']):
                message = str(row['ä¹°å®¶ç•™è¨€']).strip()
                # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
                for exact_keyword in exact_match_keywords:
                    if exact_keyword == message:
                        return True
                # æ£€æŸ¥åŒ…å«å…³é”®è¯
                message_lower = message.lower()
                for keyword in keywords:
                    if keyword.lower() in message_lower:
                        return True
            
            # æ£€æŸ¥å¤‡æ³¨å­—æ®µ
            if 'å¤‡æ³¨' in row.index and pd.notna(row['å¤‡æ³¨']):
                note = str(row['å¤‡æ³¨']).strip()
                # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
                for exact_keyword in exact_match_keywords:
                    if exact_keyword == note:
                        return True
                # æ£€æŸ¥åŒ…å«å…³é”®è¯
                note_lower = note.lower()
                for keyword in keywords:
                    if keyword.lower() in note_lower:
                        return True
            
            return False
        
        # åº”ç”¨åˆ·å•è¿‡æ»¤
        brushing_mask = df.apply(is_brushing_order, axis=1)
        df = df[~brushing_mask]
        after_brushing_filter = len(df)
        
        logger.info(f"ğŸ“Š åˆ·å•è¿‡æ»¤: {initial_count} -> {after_amount_filter} (é‡‘é¢è¿‡æ»¤) -> {after_brushing_filter} (å®¢æœå¤‡æ³¨è¿‡æ»¤)")
        if initial_count > after_brushing_filter:
            filtered_count = brushing_mask.sum()
            logger.info(f"ğŸ“Š å®¢æœå¤‡æ³¨åˆ·å•è¿‡æ»¤: è¿‡æ»¤æ‰ {filtered_count} æ¡è®°å½•")
        
        # 3. è®¢å•çŠ¶æ€è¿‡æ»¤ - åªè¿‡æ»¤æ‰ï¼šæœªä»˜æ¬¾ã€å·²å–æ¶ˆã€å·²é€€è´§
        invalid_status = ['æœªä»˜æ¬¾', 'å·²å–æ¶ˆ', 'å·²é€€è´§']
        before_filter = len(df)
        df = df[~df['è®¢å•çŠ¶æ€'].isin(invalid_status)]
        after_filter = len(df)
        logger.info(f"ğŸ“Š è®¢å•çŠ¶æ€è¿‡æ»¤: {before_filter} -> {after_filter} (è¿‡æ»¤æ‰: {invalid_status})")
        
        # 4. è¿‡æ»¤çº¿ä¸Šåº—é“º
        initial_count = len(df)
        df = df[df[SHOP_COL].apply(is_online_shop)]
        logger.info(f"ğŸ“Š çº¿ä¸Šåº—é“ºè¿‡æ»¤: {initial_count} -> {len(df)}")
        
        # 5. è¿‡æ»¤äº”å¤§æ¸ é“
        initial_count = len(df)
        def is_five_channel(shop):
            if not isinstance(shop, str):
                return False
            five_channels = ['äº¬ä¸œ', 'å¤©çŒ«', 'æ‹¼å¤šå¤š', 'æŠ–éŸ³', 'å¡è¨å¸']
            return any(channel in shop for channel in five_channels)
        df = df[df[SHOP_COL].apply(is_five_channel)]
        logger.info(f"ğŸ“Š äº”å¤§æ¸ é“è¿‡æ»¤: {initial_count} -> {len(df)}")
        
        # 6. è½¬æ¢æ—¶é—´æ ¼å¼
        if df[DATE_COL].dtype == 'object':
            logger.info("ğŸ”„ æ£€æµ‹åˆ°objectæ ¼å¼ï¼Œè½¬æ¢ä¸ºdatetime...")
            df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')
        
        # 7. è¿‡æ»¤æ˜¨æ—¥æ•°æ®
        initial_count = len(df)
        report_date = df[DATE_COL].dt.date.iloc[0] if not df.empty else None
        if report_date:
            df = df[df[DATE_COL].dt.date == report_date]
        logger.info(f"ğŸ“Š æ˜¨æ—¥æ•°æ®è¿‡æ»¤: {initial_count} -> {len(df)}")
        
        # 8. æ·»åŠ æ•°æ®æ¥æºæ ‡è¯†ï¼ˆERPæ•°æ®ï¼‰
        df['æ•°æ®æ¥æº'] = 'ERP'
        
        # 9. å¤„ç†å¤©çŒ«åˆ†é”€æ•°æ®
        if df_tianmao_fenxiao is not None and len(df_tianmao_fenxiao) > 0:
            logger.info(f"ğŸ“Š å¤„ç†å¤©çŒ«åˆ†é”€æ•°æ®: {len(df_tianmao_fenxiao)}è¡Œ")
            df_tianmao_fenxiao = check_and_fix_column_names(df_tianmao_fenxiao)
            
            # ç¡®ä¿å¤©çŒ«åˆ†é”€æ•°æ®ä¹Ÿè¿›è¡Œè®¢å•çŠ¶æ€è¿‡æ»¤
            df_tianmao_fenxiao = df_tianmao_fenxiao[~df_tianmao_fenxiao['è®¢å•çŠ¶æ€'].isin(invalid_status)]
            
            # ä¸ºåˆ†é”€æ•°æ®æ·»åŠ å®¢æœå¤‡æ³¨åˆ·å•ç­›é€‰
            def filter_fenxiao_brushing(df_fenxiao):
                """ä¸ºåˆ†é”€æ•°æ®æ·»åŠ åˆ·å•ç­›é€‰"""
                if df_fenxiao.empty:
                    return df_fenxiao
                
                keywords = ['æŠ½çº¸', 'çº¸å·¾', 'åˆ·å•', 'æµ‹è¯•', 'è™šæ‹Ÿ']
                exact_match_keywords = ['å®Œå…¨=ä¸å‘è´§', 'ä¸å‘è´§']  # ç²¾ç¡®åŒ¹é…
                
                # æ£€æŸ¥è®¢å•å¤‡æ³¨å’Œä¹°å®¶ç•™è¨€å­—æ®µ
                def is_brushing_fenxiao(row):
                    fields_to_check = []
                    
                    # æ”¶é›†æ‰€æœ‰éœ€è¦æ£€æŸ¥çš„å­—æ®µ
                    if 'è®¢å•å¤‡æ³¨' in row.index and pd.notna(row['è®¢å•å¤‡æ³¨']):
                        fields_to_check.append(str(row['è®¢å•å¤‡æ³¨']).strip())
                    if 'ä¹°å®¶ç•™è¨€' in row.index and pd.notna(row['ä¹°å®¶ç•™è¨€']):
                        fields_to_check.append(str(row['ä¹°å®¶ç•™è¨€']).strip())
                    if 'å¤‡æ³¨' in row.index and pd.notna(row['å¤‡æ³¨']):
                        fields_to_check.append(str(row['å¤‡æ³¨']).strip())
                    
                    # æ£€æŸ¥æ¯ä¸ªå­—æ®µ
                    for field in fields_to_check:
                        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
                        for exact_keyword in exact_match_keywords:
                            if exact_keyword == field:
                                return True
                        
                        # æ£€æŸ¥åŒ…å«å…³é”®è¯
                        field_lower = field.lower()
                        for keyword in keywords:
                            if keyword.lower() in field_lower:
                                return True
                    
                    return False
                
                brushing_mask = df_fenxiao.apply(is_brushing_fenxiao, axis=1)
                filtered_df = df_fenxiao[~brushing_mask]
                
                if brushing_mask.sum() > 0:
                    logger.info(f"ğŸ“Š åˆ†é”€æ•°æ®åˆ·å•è¿‡æ»¤: è¿‡æ»¤æ‰ {brushing_mask.sum()} æ¡è®°å½•")
                
                return filtered_df
            
            df_tianmao_fenxiao = filter_fenxiao_brushing(df_tianmao_fenxiao)
            logger.info(f"ğŸ“Š å¤©çŒ«åˆ†é”€æ•°æ®è¿‡æ»¤å: {len(df_tianmao_fenxiao)}è¡Œ")
            
            # ç¡®ä¿æ•°æ®æ¥æºæ ‡è¯†
            df_tianmao_fenxiao['æ•°æ®æ¥æº'] = 'åˆ†é”€'
            
            # åˆå¹¶å¤©çŒ«åˆ†é”€æ•°æ®åˆ°ä¸»æ•°æ®
            df = pd.concat([df, df_tianmao_fenxiao], ignore_index=True)
            logger.info(f"ğŸ“Š åˆå¹¶å¤©çŒ«åˆ†é”€æ•°æ®åæ€»è¡Œæ•°: {len(df)}")
        
        # 10. å¤„ç†äº¬ä¸œåˆ†é”€æ•°æ®
        if df_fenxiao is not None and len(df_fenxiao) > 0:
            logger.info(f"ğŸ“Š å¤„ç†äº¬ä¸œåˆ†é”€æ•°æ®: {len(df_fenxiao)}è¡Œ")
            df_fenxiao = check_and_fix_column_names(df_fenxiao)
            
            # ç¡®ä¿äº¬ä¸œåˆ†é”€æ•°æ®ä¹Ÿè¿›è¡Œè®¢å•çŠ¶æ€è¿‡æ»¤
            df_fenxiao = df_fenxiao[~df_fenxiao['è®¢å•çŠ¶æ€'].isin(invalid_status)]
            
            # ä¸ºäº¬ä¸œåˆ†é”€æ•°æ®æ·»åŠ å®¢æœå¤‡æ³¨åˆ·å•ç­›é€‰
            df_fenxiao = filter_fenxiao_brushing(df_fenxiao)
            logger.info(f"ğŸ“Š äº¬ä¸œåˆ†é”€æ•°æ®è¿‡æ»¤å: {len(df_fenxiao)}è¡Œ")
            
            # ç¡®ä¿æ•°æ®æ¥æºæ ‡è¯†
            df_fenxiao['æ•°æ®æ¥æº'] = 'åˆ†é”€'
            
            # åˆå¹¶äº¬ä¸œåˆ†é”€æ•°æ®åˆ°ä¸»æ•°æ®
            df = pd.concat([df, df_fenxiao], ignore_index=True)
            logger.info(f"ğŸ“Š åˆå¹¶äº¬ä¸œåˆ†é”€æ•°æ®åæ€»è¡Œæ•°: {len(df)}")
        
        # 11. æœ€ç»ˆæ•°æ®ç±»å‹è½¬æ¢
        df[AMOUNT_COL] = pd.to_numeric(df[AMOUNT_COL], errors='coerce').fillna(0)
        df[QTY_COL] = pd.to_numeric(df[QTY_COL], errors='coerce').fillna(0)
        
        logger.info(f"ğŸ“Š æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ - {AMOUNT_COL}: {df[AMOUNT_COL].dtype}, {QTY_COL}: {df[QTY_COL].dtype}")
        logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ•°æ®è¡Œæ•°: {len(df)}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        return pd.DataFrame()

# ========== æŠ¥è¡¨ç”Ÿæˆä¸æ¨é€ ==========
# æ¸ é“å½’ç±»å‡½æ•°
CHANNEL_NAME_MAP = {
    "å¡è¨å¸æ¸ é“": "å¡è¨å¸æ¸ é“",
    "å°çº¢ä¹¦": "å¡è¨å¸æ¸ é“",
    "å¤©çŒ«æ¸ é“": "å¤©çŒ«æ¸ é“",
    "æ·˜å®æ¸ é“": "å¤©çŒ«æ¸ é“",
    "äº¬ä¸œæ¸ é“": "äº¬ä¸œæ¸ é“",
    "æ‹¼å¤šå¤šæ¸ é“": "æ‹¼å¤šå¤šæ¸ é“",
    "æŠ–éŸ³æ¸ é“": "æŠ–éŸ³æ¸ é“",
    "å¿«æ‰‹æ¸ é“": "æŠ–éŸ³æ¸ é“"
}
def classify_channel(shop_name):
    if not isinstance(shop_name, str):
        return "å…¶ä»–"
    # å¡è¨å¸ä¼˜å…ˆ
    if any(kw in shop_name for kw in CHANNEL_GROUPS['å¡è¨å¸æ¸ é“']['keywords']):
        return "å¡è¨å¸æ¸ é“"
    if any(kw in shop_name for kw in CHANNEL_GROUPS['äº¬ä¸œæ¸ é“']['keywords']):
        return "äº¬ä¸œæ¸ é“"
    if any(kw in shop_name for kw in CHANNEL_GROUPS['å¤©çŒ«æ¸ é“']['keywords']) and not any(kw in shop_name for kw in CHANNEL_GROUPS['å¡è¨å¸æ¸ é“']['keywords']):
        return "å¤©çŒ«æ¸ é“"
    if any(kw in shop_name for kw in CHANNEL_GROUPS['æ‹¼å¤šå¤šæ¸ é“']['keywords']):
        return "æ‹¼å¤šå¤šæ¸ é“"
    if any(kw in shop_name for kw in CHANNEL_GROUPS['æŠ–éŸ³æ¸ é“']['keywords']):
        return "æŠ–éŸ³æ¸ é“"
    return "å…¶ä»–"

def generate_group_report(group_name, group_type, keywords, df, df_prev, report_date):
    if group_type == 'business':
        # ä¿®å¤äº‹ä¸šéƒ¨ç­›é€‰é€»è¾‘ï¼šå¯¹äºåˆ†é”€æ•°æ®ä½¿ç”¨å“ç±»åˆ—ï¼Œå¯¹äºERPæ•°æ®ä½¿ç”¨è´§å“åç§°åˆ—
        def business_filter(row):
            # å¦‚æœæ˜¯åˆ†é”€æ•°æ®ï¼Œä½¿ç”¨å“ç±»åˆ—ç­›é€‰
            if 'æ•°æ®æ¥æº' in row and row['æ•°æ®æ¥æº'] == 'åˆ†é”€' and 'å“ç±»' in row:
                return any(kw in str(row['å“ç±»']) for kw in keywords)
            # å¦‚æœæ˜¯ERPæ•°æ®ï¼Œä½¿ç”¨è´§å“åç§°åˆ—ç­›é€‰
            else:
                return any(kw in str(row[CATEGORY_COL]) for kw in keywords)
        
        group_df = df[df.apply(business_filter, axis=1)]
        prev_group_df = df_prev[df_prev.apply(business_filter, axis=1)] if df_prev is not None else None
    else:
        # æ¸ é“åˆ†ç»„ï¼Œå¡è¨å¸ä¼˜å…ˆ
        if group_name == 'å¡è¨å¸æ¸ é“':
            group_df = df[df[SHOP_COL].apply(lambda x: any(kw in str(x) for kw in CHANNEL_GROUPS['å¡è¨å¸æ¸ é“']['keywords']))]
            prev_group_df = df_prev[df_prev[SHOP_COL].apply(lambda x: any(kw in str(x) for kw in CHANNEL_GROUPS['å¡è¨å¸æ¸ é“']['keywords']))] if df_prev is not None else None
        elif group_name == 'å¤©çŒ«æ¸ é“':
            group_df = df[df[SHOP_COL].apply(lambda x: (any(kw in str(x) for kw in CHANNEL_GROUPS['å¤©çŒ«æ¸ é“']['keywords'])) and not any(kw in str(x) for kw in CHANNEL_GROUPS['å¡è¨å¸æ¸ é“']['keywords']))]
            prev_group_df = df_prev[df_prev[SHOP_COL].apply(lambda x: (any(kw in str(x) for kw in CHANNEL_GROUPS['å¤©çŒ«æ¸ é“']['keywords'])) and not any(kw in str(x) for kw in CHANNEL_GROUPS['å¡è¨å¸æ¸ é“']['keywords']))] if df_prev is not None else None
        else:
            group_df = df[df[SHOP_COL].apply(lambda x: any(kw in str(x) for kw in CHANNEL_GROUPS[group_name]['keywords']))]
            prev_group_df = df_prev[df_prev[SHOP_COL].apply(lambda x: any(kw in str(x) for kw in CHANNEL_GROUPS[group_name]['keywords']))] if df_prev is not None else None
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logger.info(f"ğŸ” {group_name} åŒ¹é…æ•°æ®é‡: {len(group_df)} è¡Œ")
    if len(group_df) > 0:
        logger.info(f"   ğŸ“‹ å…³é”®è¯: {keywords}")
        if group_type == 'business':
            logger.info(f"   ğŸ·ï¸ åŒ¹é…çš„å“ç±»: {group_df[CATEGORY_COL].unique()[:5].tolist()}")
        else:
            logger.info(f"   ğŸª åŒ¹é…çš„åº—é“º: {group_df[SHOP_COL].unique()[:5].tolist()}")
    
    if group_df.empty:
        # æ²¡æœ‰æ•°æ®æ—¶ç”Ÿæˆç©ºæ•°æ®é¡µé¢
        content = f"ğŸ¢ {group_name}æ—¥æŠ¥\nğŸ“… æ•°æ®æ—¥æœŸ: {report_date}\n\nâš ï¸ ä»Šæ—¥æš‚æ— é”€å”®æ•°æ®"
        title_cn = f"{group_name}æ—¥æŠ¥ï¼ˆ{report_date}ï¼‰"
        empty_content = f"""ğŸ¢ {group_name}æ—¥æŠ¥
ğŸ“… æ•°æ®æ—¥æœŸ: {report_date}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ä»Šæ—¥æš‚æ— é”€å”®æ•°æ®
è¯¥äº‹ä¸šéƒ¨/æ¸ é“ä»Šæ—¥æš‚æ— é”€å”®æ•°æ®ï¼Œè¯·æŸ¥çœ‹å…¶ä»–æ—¶é—´æ®µçš„æŠ¥å‘Šã€‚"""
        
        html_content = generate_html_content(title_cn, empty_content)
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        logger.info(f"ğŸ”§ {group_name} ç©ºæ•°æ®é¡µé¢HTMLç”Ÿæˆè°ƒè¯•ä¿¡æ¯:")
        logger.info(f"   å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
        logger.info(f"   ä½¿ç”¨ç»Ÿä¸€HTMLç”Ÿæˆå‡½æ•°: âœ…")
        
        file_prefix = to_pinyin(group_name)
        filename = f"{file_prefix}_{report_date}.html".replace('/', '-')
        script_dir = os.path.dirname(os.path.abspath(__file__))
        reports_dir = os.path.join(script_dir, "reports")
        os.makedirs(reports_dir, exist_ok=True)
        filepath = os.path.join(reports_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(html_content)
        return content, filename
    
    # æœ‰æ•°æ®æ—¶çš„æ­£å¸¸å¤„ç†é€»è¾‘
    # å…ˆæ¸…ç†æ•°æ®ï¼Œç¡®ä¿é‡‘é¢å’Œæ•°é‡å­—æ®µæ˜¯æœ‰æ•ˆçš„æ•°å­—
    group_df_clean = group_df.copy()
    
    # ç¡®ä¿é‡‘é¢å’Œæ•°é‡å­—æ®µä¸ºæ•°å€¼ç±»å‹
    if group_df_clean[AMOUNT_COL].dtype == 'object':
        group_df_clean[AMOUNT_COL] = pd.to_numeric(group_df_clean[AMOUNT_COL], errors='coerce').fillna(0)
    if group_df_clean[QTY_COL].dtype == 'object':
        group_df_clean[QTY_COL] = pd.to_numeric(group_df_clean[QTY_COL], errors='coerce').fillna(0)
    
    # æ•´ä½“æ•°æ®ï¼ˆç°åœ¨åŒ…å«äº†äº¬ä¸œåˆ†é”€æ•°æ®ï¼‰
    total_amount = int(group_df_clean[AMOUNT_COL].sum())
    total_qty = int(group_df_clean[QTY_COL].sum())
    avg_price = int(total_amount / total_qty) if total_qty > 0 else 0
    
    prev_amount = 0
    prev_qty = 0
    if prev_group_df is not None and not prev_group_df.empty:
        prev_group_df_clean = prev_group_df.copy()
        # ç¡®ä¿åŒæœŸæ•°æ®ä¹Ÿæ˜¯æ•°å€¼ç±»å‹
        if prev_group_df_clean[AMOUNT_COL].dtype == 'object':
            prev_group_df_clean[AMOUNT_COL] = pd.to_numeric(prev_group_df_clean[AMOUNT_COL], errors='coerce').fillna(0)
        if prev_group_df_clean[QTY_COL].dtype == 'object':
            prev_group_df_clean[QTY_COL] = pd.to_numeric(prev_group_df_clean[QTY_COL], errors='coerce').fillna(0)
        prev_amount = int(prev_group_df_clean[AMOUNT_COL].sum())
        prev_qty = int(prev_group_df_clean[QTY_COL].sum())
    
    prev_avg_price = int(prev_amount / prev_qty) if prev_qty > 0 else 0
    
    # åˆ†é”€æ•°æ®ç»Ÿè®¡ï¼ˆåŒ…å«å¤©çŒ«åˆ†é”€å’Œäº¬ä¸œåˆ†é”€ï¼‰
    fenxiao_amount = 0
    fenxiao_qty = 0
    if 'æ•°æ®æ¥æº' in group_df_clean.columns:
        fenxiao_df = group_df_clean[group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€']
        if not fenxiao_df.empty:
            fenxiao_amount = fenxiao_df[AMOUNT_COL].sum()
            fenxiao_qty = fenxiao_df[QTY_COL].sum()
            logger.info(f"ğŸ“Š {group_name} åˆ†é”€æ•°æ®: é‡‘é¢={fenxiao_amount}, æ•°é‡={fenxiao_qty}")
    
    # åŒæœŸåˆ†é”€æ•°æ®
    prev_fenxiao_amount = 0
    prev_fenxiao_qty = 0
    if prev_group_df is not None and 'æ•°æ®æ¥æº' in prev_group_df.columns:
        prev_fenxiao_df = prev_group_df_clean[prev_group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€']
        if not prev_fenxiao_df.empty:
            prev_fenxiao_amount = prev_fenxiao_df[AMOUNT_COL].sum()
            prev_fenxiao_qty = prev_fenxiao_df[QTY_COL].sum()
    
    # å“ç±»æ˜ç»†
    category_data = group_df_clean.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
    category_data = category_data.sort_values(AMOUNT_COL, ascending=False)
    prev_category_data = None
    if prev_group_df is not None:
        prev_category_data = prev_group_df_clean.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
    
    # å“ç±»åˆ†é”€æ•°æ®
    category_fenxiao_data = None
    if 'æ•°æ®æ¥æº' in group_df_clean.columns:
        fenxiao_df = group_df_clean[group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€']
        if not fenxiao_df.empty:
            category_fenxiao_data = fenxiao_df.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
            category_fenxiao_data = category_fenxiao_data.sort_values(AMOUNT_COL, ascending=False)
    
    # æ¸ é“æ•°æ®ï¼ˆä»…äº‹ä¸šéƒ¨ï¼ŒæŒ‰äº”å¤§æ¸ é“èšåˆï¼‰
    channel_data = None
    prev_channel_data = None
    if group_type == 'business':
        group_df_clean['æ¸ é“'] = group_df_clean[SHOP_COL].apply(classify_channel)
        channel_data = group_df_clean.groupby('æ¸ é“').agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
        channel_data = channel_data[channel_data['æ¸ é“'].isin(CHANNEL_GROUPS.keys())]
        channel_data = channel_data.sort_values(AMOUNT_COL, ascending=False)
        if prev_group_df is not None:
            prev_group_df_clean['æ¸ é“'] = prev_group_df_clean[SHOP_COL].apply(classify_channel)
            prev_channel_data = prev_group_df_clean.groupby('æ¸ é“').agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
    
    # æ¸ é“åˆ†é”€æ•°æ®
    channel_fenxiao_data = None
    if group_type == 'business' and 'æ•°æ®æ¥æº' in group_df_clean.columns:
        fenxiao_df = group_df_clean[group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€']
        if not fenxiao_df.empty:
            fenxiao_df['æ¸ é“'] = fenxiao_df[SHOP_COL].apply(classify_channel)
            channel_fenxiao_data = fenxiao_df.groupby('æ¸ é“').agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
            channel_fenxiao_data = channel_fenxiao_data.sort_values(AMOUNT_COL, ascending=False)
            logger.info(f"ğŸ“Š æ¸ é“åˆ†é”€æ•°æ®: {len(channel_fenxiao_data)}ä¸ªæ¸ é“")
            for _, row in channel_fenxiao_data.iterrows():
                logger.info(f"   {row['æ¸ é“']}: Â¥{row[AMOUNT_COL]:,}, {row[QTY_COL]}ä»¶")
    
    # åº—é“ºæ•°æ® - åœ¨group_dfè¢«ä¿®æ”¹ä¹‹å‰ç”Ÿæˆ
    shop_data = group_df_clean.groupby(SHOP_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
    shop_data = shop_data.sort_values(AMOUNT_COL, ascending=False)
    prev_shop_data = None
    if prev_group_df is not None:
        prev_shop_data = prev_group_df_clean.groupby(SHOP_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
    
    # åº—é“ºåˆ†é”€æ•°æ®
    shop_fenxiao_data = None
    if 'æ•°æ®æ¥æº' in group_df_clean.columns:
        fenxiao_df = group_df_clean[group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€']
        if not fenxiao_df.empty:
            shop_fenxiao_data = fenxiao_df.groupby(SHOP_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
            shop_fenxiao_data = shop_fenxiao_data.sort_values(AMOUNT_COL, ascending=False)
    
    # TOPå•å“
    product_data = group_df_clean.groupby(MODEL_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
    # ä½¿ç”¨æ•°å€¼æ¯”è¾ƒ
    product_data = product_data[product_data[AMOUNT_COL] > 1000]  # é˜ˆå€¼ä»5000é™åˆ°1000
    product_data = product_data.sort_values(AMOUNT_COL, ascending=False)
    
    # å¯¹TOPå•å“åº”ç”¨äº§å“åŒ¹é…é€»è¾‘
    if not product_data.empty and 'æ•°æ®æ¥æº' in group_df_clean.columns:
        logger.info("ğŸ”„ å¯¹TOPå•å“åº”ç”¨äº§å“åŒ¹é…é€»è¾‘...")
        # è·å–åˆ†é”€æ•°æ®çš„äº§å“åŒ¹é…ä¿¡æ¯
        fenxiao_df = group_df_clean[group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€']
        if not fenxiao_df.empty:
            # åˆ›å»ºäº§å“åç§°æ˜ å°„å­—å…¸
            product_mapping = {}
            for _, row in fenxiao_df.iterrows():
                original_name = row.get('è´§å“åç§°', '')
                matched_name = row.get('è§„æ ¼åç§°', '')
                if original_name and matched_name and original_name != matched_name:
                    product_mapping[original_name] = matched_name
            
            # åº”ç”¨æ˜ å°„åˆ°TOPå•å“æ•°æ®
            if product_mapping:
                logger.info(f"ğŸ“Š äº§å“åŒ¹é…æ˜ å°„: {len(product_mapping)}ä¸ª")
                for original, matched in list(product_mapping.items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    logger.info(f"   {original} -> {matched}")
                
                # æ›´æ–°TOPå•å“ä¸­çš„äº§å“åç§°
                for index, row in product_data.iterrows():
                    model_name = row[MODEL_COL]
                    if model_name in product_mapping:
                        product_data.at[index, MODEL_COL] = product_mapping[model_name]
                        logger.info(f"   âœ… TOPå•å“åŒ¹é…: {model_name} -> {product_mapping[model_name]}")
    
    # é‡æ–°æŒ‰åŒ¹é…åçš„åç§°èšåˆTOPå•å“æ•°æ®
    if not product_data.empty:
        product_data = product_data.groupby(MODEL_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
        product_data = product_data.sort_values(AMOUNT_COL, ascending=False)
    
    # ---------- ç»Ÿä¸€åˆ†æ®µç‰ˆ ----------
    web_content = f"""ğŸ¢ {group_name}æ—¥æŠ¥
ğŸ“… æ•°æ®æ—¥æœŸ: {report_date}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š æ•´ä½“æ•°æ®
æ€»é”€å”®é¢: Â¥{total_amount:,}ï¼ˆç¯æ¯”:{calculate_ratio(total_amount, prev_amount)}ï¼‰

å¹³å‡å•ä»·: Â¥{avg_price:,}ï¼ˆç¯æ¯”:{calculate_ratio(avg_price, prev_avg_price)}ï¼‰"""

    # æ·»åŠ åˆ†é”€æ•°æ®åˆ°æ•´ä½“æ•°æ®
    if fenxiao_amount > 0:
        web_content += f"""
å…¶ä¸­åˆ†é”€é”€å”®: Â¥{fenxiao_amount:,}ï¼ˆç¯æ¯”:{calculate_ratio(fenxiao_amount, prev_fenxiao_amount)})"""

    web_content += "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“‹ å“ç±»æ˜ç»†"
    
    for _, row in category_data.iterrows():
        cat = row[CATEGORY_COL]
        amount = to_number(row[AMOUNT_COL])
        qty = to_number(row[QTY_COL])
        price = int(amount / qty) if qty else 0
        prev_amount_cat = to_number(prev_category_data.loc[prev_category_data[CATEGORY_COL] == cat, AMOUNT_COL].sum()) if prev_category_data is not None else 0
        web_content += f"\nâ€¢ {cat}: Â¥{amount:,} ({calculate_ratio(amount, prev_amount_cat)}) | å•ä»·: Â¥{price:,}"
        
        # æ·»åŠ å“ç±»åˆ†é”€æ•°æ®
        if category_fenxiao_data is not None:
            cat_fenxiao = category_fenxiao_data[category_fenxiao_data[CATEGORY_COL] == cat]
            if not cat_fenxiao.empty:
                fenxiao_amt = to_number(cat_fenxiao.iloc[0][AMOUNT_COL])
                fenxiao_qty_cat = to_number(cat_fenxiao.iloc[0][QTY_COL])
                fenxiao_price = int(fenxiao_amt / fenxiao_qty_cat) if fenxiao_qty_cat else 0
                web_content += f"\nå…¶ä¸­åˆ†é”€é”€å”®: Â¥{fenxiao_amt:,} ({calculate_ratio(fenxiao_amt, 0)}) | å•ä»·: Â¥{fenxiao_price:,}"

    if group_type == 'business' and channel_data is not None and not channel_data.empty:
        web_content += "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸª æ¸ é“æ•°æ®"
        for _, row in channel_data.iterrows():
            channel = row['æ¸ é“']
            amount = to_number(row[AMOUNT_COL])
            qty = to_number(row[QTY_COL])
            price = int(amount / qty) if qty else 0
            prev_amount_channel = to_number(prev_channel_data.loc[prev_channel_data['æ¸ é“'] == channel, AMOUNT_COL].sum()) if prev_channel_data is not None else 0
            web_content += f"\nâ€¢ {channel}: Â¥{amount:,} ({calculate_ratio(amount, prev_amount_channel)}) | å•ä»·: Â¥{price:,}"
            
            # æ·»åŠ æ¸ é“åˆ†é”€æ•°æ®
            if channel_fenxiao_data is not None:
                channel_fenxiao = channel_fenxiao_data[channel_fenxiao_data['æ¸ é“'] == channel]
                if not channel_fenxiao.empty:
                    fenxiao_amt = to_number(channel_fenxiao.iloc[0][AMOUNT_COL])
                    fenxiao_qty_channel = to_number(channel_fenxiao.iloc[0][QTY_COL])
                    fenxiao_price = int(fenxiao_amt / fenxiao_qty_channel) if fenxiao_qty_channel else 0
                    web_content += f"\nå…¶ä¸­åˆ†é”€é”€å”®: Â¥{fenxiao_amt:,} ({calculate_ratio(fenxiao_amt, 0)}) | å•ä»·: Â¥{fenxiao_price:,}"
            
            # æ·»åŠ æ¸ é“å†…å“ç±»ç»†åˆ†
            channel_df = group_df_clean[group_df_clean[SHOP_COL].apply(lambda x: any(kw in str(x) for kw in CHANNEL_GROUPS[channel]['keywords']))]
            if not channel_df.empty:
                channel_category_data = channel_df.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
                channel_category_data = channel_category_data.sort_values(AMOUNT_COL, ascending=False)
                
                if not channel_category_data.empty:
                    category_breakdown = []
                    for _, cat_row in channel_category_data.head(3).iterrows():  # åªæ˜¾ç¤ºå‰3ä¸ªå“ç±»
                        cat_name = cat_row[CATEGORY_COL]
                        cat_amount = to_number(cat_row[AMOUNT_COL])
                        category_breakdown.append(f"{cat_name} Â¥{cat_amount:,}")
                    
                    if category_breakdown:
                        web_content += f"\nç»†åˆ†å“ç±»: {', '.join(category_breakdown)}"
                
                # æ·»åŠ æ¸ é“åˆ†é”€å“ç±»ç»†åˆ†
                if channel_fenxiao_data is not None and not channel_fenxiao.empty:
                    channel_fenxiao_df = group_df_clean[(group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€') & 
                                                       (group_df_clean[SHOP_COL].apply(lambda x: any(kw in str(x) for kw in CHANNEL_GROUPS[channel]['keywords'])))]
                    if not channel_fenxiao_df.empty:
                        fenxiao_category_data = channel_fenxiao_df.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
                        fenxiao_category_data = fenxiao_category_data.sort_values(AMOUNT_COL, ascending=False)
                        
                        if not fenxiao_category_data.empty:
                            fenxiao_category_breakdown = []
                            for _, cat_row in fenxiao_category_data.head(3).iterrows():  # åªæ˜¾ç¤ºå‰3ä¸ªå“ç±»
                                cat_name = cat_row[CATEGORY_COL]
                                cat_amount = to_number(cat_row[AMOUNT_COL])
                                fenxiao_category_breakdown.append(f"{cat_name} Â¥{cat_amount:,}")
                            
                            if fenxiao_category_breakdown:
                                web_content += f"\nåˆ†é”€å“ç±»: {', '.join(fenxiao_category_breakdown)}"

    if shop_data is not None and not shop_data.empty:
        web_content += "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸª åº—é“ºæ•°æ®"
        for _, row in shop_data.head(10).iterrows():
            shop = row[SHOP_COL]
            amount = to_number(row[AMOUNT_COL])
            qty = to_number(row[QTY_COL])
            prev_amount_shop = to_number(prev_shop_data.loc[prev_shop_data[SHOP_COL] == shop, AMOUNT_COL].sum()) if prev_shop_data is not None else 0
            web_content += f"\nâ€¢ {shop}: Â¥{amount:,} ({calculate_ratio(amount, prev_amount_shop)})"
            
            # æ·»åŠ åº—é“ºåˆ†é”€æ•°æ®
            if shop_fenxiao_data is not None:
                shop_fenxiao = shop_fenxiao_data[shop_fenxiao_data[SHOP_COL] == shop]
                if not shop_fenxiao.empty:
                    fenxiao_amt = to_number(shop_fenxiao.iloc[0][AMOUNT_COL])
                    fenxiao_qty_shop = to_number(shop_fenxiao.iloc[0][QTY_COL])
                    web_content += f"\nå…¶ä¸­åˆ†é”€é”€å”®: Â¥{fenxiao_amt:,} ({calculate_ratio(fenxiao_amt, 0)})"

    if product_data is not None and not product_data.empty:
        web_content += "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ† TOP å•å“"
        for _, row in product_data.head(10).iterrows():
            model = row[MODEL_COL]
            amount = to_number(row[AMOUNT_COL])
            qty = to_number(row[QTY_COL])
            price = int(amount / qty) if qty else 0
            # è®¡ç®—ç¯æ¯”æ•°æ®
            prev_amount_product = 0
            if df_prev is not None:
                prev_product = df_prev[df_prev[MODEL_COL] == model]
                if not prev_product.empty:
                    prev_amount_product = int(to_number(prev_product[AMOUNT_COL].sum()))
            ratio_str = f"ï¼Œç¯æ¯” {calculate_ratio(amount, prev_amount_product)}"
            web_content += f"\nâ€¢ {model}: Â¥{int(amount):,} | å•ä»·: Â¥{price:,}{ratio_str}"

    web_content += "\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nğŸ“‹ åº—é“ºå•å“æ˜ç»†"
    if shop_data is not None and not shop_data.empty:
        for _, shop_row in shop_data.head(10).iterrows():
            shop = shop_row[SHOP_COL]
            shop_products = df[df[SHOP_COL] == shop].groupby(MODEL_COL).agg({
                AMOUNT_COL: 'sum', QTY_COL: 'sum'
            }).reset_index()
            shop_products = shop_products[(shop_products[AMOUNT_COL] > 100) & ~shop_products[MODEL_COL].str.contains('è¿è´¹|å¤–æœº|è™šæ‹Ÿ|èµ å“')]
            shop_products = shop_products.sort_values(AMOUNT_COL, ascending=False).head(5)
            web_content += f"\n\nğŸª ã€{shop}ã€‘æ ¸å¿ƒäº§å“"
            if not shop_products.empty:
                for _, p in shop_products.iterrows():
                    price = int(p[AMOUNT_COL] / p[QTY_COL]) if p[QTY_COL] else 0
                    # è®¡ç®—ç¯æ¯”æ•°æ®
                    prev_amount_product = 0
                    if df_prev is not None:
                        prev_product = df_prev[(df_prev[MODEL_COL] == p[MODEL_COL]) & (df_prev[SHOP_COL] == shop)]
                        if not prev_product.empty:
                            prev_amount_product = int(prev_product[AMOUNT_COL].sum())
                    ratio_str = f"ï¼Œç¯æ¯” {calculate_ratio(p[AMOUNT_COL], prev_amount_product)}"
                    web_content += f"\nâ”œâ”€ ğŸ”¸ {p[MODEL_COL]}"
                    web_content += f"\nâ”œâ”€ é”€å”®é¢: Â¥{int(p[AMOUNT_COL]):,} | å•ä»·: Â¥{price:,}{ratio_str}"
    else:
        web_content += "\n  æš‚æ— åº—é“ºæ•°æ®"

    # ç”Ÿæˆçº¯æ–‡æœ¬ç‰ˆæœ¬ç”¨äºå¾®ä¿¡å‘é€ - ç®€åŒ–å†…å®¹ï¼Œç§»é™¤åº—é“ºå•å“æ˜ç»†ï¼Œä¿æŒåŸæœ‰æ ¼å¼
    content = f"ğŸ¢ {group_name}æ—¥æŠ¥\nğŸ“… æ•°æ®æ—¥æœŸ: {report_date}\n\n"
    content += f"ğŸ“Š æ•´ä½“æ•°æ®\næ€»é”€å”®é¢: Â¥{total_amount:,}ï¼ˆç¯æ¯”:{calculate_ratio(total_amount, prev_amount)}ï¼‰\næ€»é”€é‡: {total_qty}ä»¶ï¼ˆç¯æ¯”:{calculate_ratio(total_qty, prev_qty)}ï¼‰\nå¹³å‡å•ä»·: Â¥{avg_price:,}"
    
    # æ·»åŠ åˆ†é”€æ•°æ®åˆ°å¾®ä¿¡å†…å®¹
    if fenxiao_amount > 0:
        content += f"\nå…¶ä¸­åˆ†é”€é‡‘é¢: Â¥{fenxiao_amount:,}ï¼ˆç¯æ¯”:{calculate_ratio(fenxiao_amount, prev_fenxiao_amount)}ï¼‰"
    
    content += "\n\n"
    
    # å“ç±»æ˜ç»† - åªæ˜¾ç¤ºå‰3ä¸ª
    content += "ğŸ“‹ å“ç±»æ˜ç»†\n"
    for i, (_, row) in enumerate(category_data.iterrows()):
        if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå“ç±»
            break
        cat = row[CATEGORY_COL]
        amount = to_number(row[AMOUNT_COL])
        qty = to_number(row[QTY_COL])
        price = int(amount / qty) if qty else 0
        prev_amount_cat = 0
        if prev_category_data is not None:
            prev_row = prev_category_data[prev_category_data[CATEGORY_COL] == cat]
            if not prev_row.empty:
                prev_amount_cat = to_number(prev_row.iloc[0][AMOUNT_COL])
        content += f"â€¢ {cat}: Â¥{amount:,} ({calculate_ratio(amount, prev_amount_cat)})ï¼Œ{qty}ä»¶\n"
        
        # æ·»åŠ å“ç±»åˆ†é”€æ•°æ®åˆ°å¾®ä¿¡å†…å®¹
        if category_fenxiao_data is not None:
            cat_fenxiao = category_fenxiao_data[category_fenxiao_data[CATEGORY_COL] == cat]
            if not cat_fenxiao.empty:
                fenxiao_amt = to_number(cat_fenxiao.iloc[0][AMOUNT_COL])
                fenxiao_qty_cat = to_number(cat_fenxiao.iloc[0][QTY_COL])
                content += f"å…¶ä¸­åˆ†é”€é‡‘é¢: Â¥{fenxiao_amt:,} ({calculate_ratio(fenxiao_amt, 0)})ï¼Œ{fenxiao_qty_cat}ä»¶\n"
    
    # æ¸ é“æ•°æ®ï¼ˆä»…äº‹ä¸šéƒ¨ï¼Œäº”å¤§æ¸ é“èšåˆï¼‰- åªæ˜¾ç¤ºå‰3ä¸ª
    if group_type == 'business' and channel_data is not None and not channel_data.empty:
        content += "\nğŸª æ¸ é“æ•°æ®\n"
        for i, (_, row) in enumerate(channel_data.iterrows()):
            if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ¸ é“
                break
            channel = row['æ¸ é“']
            amount = to_number(row[AMOUNT_COL])
            qty = to_number(row[QTY_COL])
            price = int(amount / qty) if qty else 0
            prev_amount_channel = 0
            if prev_channel_data is not None:
                prev_row = prev_channel_data[prev_channel_data['æ¸ é“'] == channel]
                if not prev_row.empty:
                    prev_amount_channel = prev_row.iloc[0][AMOUNT_COL]
            
            # è·å–è¯¥æ¸ é“çš„å“ç±»ç»†åˆ†æ•°æ®
            channel_categories = []
            channel_df = group_df_clean[group_df_clean[SHOP_COL].apply(classify_channel) == channel]
            if not channel_df.empty:
                channel_cat_data = channel_df.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
                channel_cat_data = channel_cat_data.sort_values(AMOUNT_COL, ascending=False)
                for _, cat_row in channel_cat_data.iterrows():
                    cat_name = cat_row[CATEGORY_COL]
                    cat_amount = to_number(cat_row[AMOUNT_COL])
                    cat_qty = to_number(cat_row[QTY_COL])
                    channel_categories.append(f"{cat_name} Â¥{cat_amount:,}")
            
            content += f"â€¢ {channel}: Â¥{amount:,} ({calculate_ratio(amount, prev_amount_channel)})ï¼Œ{qty}ä»¶ | å•ä»·: Â¥{price}"
            if channel_categories:
                content += f"ï¼Œ{', '.join(channel_categories[:3])}"  # åªæ˜¾ç¤ºå‰3ä¸ªå“ç±»
            content += "\n"
            
            # æ·»åŠ æ¸ é“åˆ†é”€æ•°æ®åˆ°å¾®ä¿¡å†…å®¹
            if channel_fenxiao_data is not None:
                channel_fenxiao = channel_fenxiao_data[channel_fenxiao_data['æ¸ é“'] == channel]
                if not channel_fenxiao.empty:
                    fenxiao_amt = to_number(channel_fenxiao.iloc[0][AMOUNT_COL])
                    fenxiao_qty_channel = to_number(channel_fenxiao.iloc[0][QTY_COL])
                    fenxiao_price = int(fenxiao_amt / fenxiao_qty_channel) if fenxiao_qty_channel else 0
                    
                    # è·å–è¯¥æ¸ é“åˆ†é”€çš„å“ç±»ç»†åˆ†æ•°æ®
                    channel_fenxiao_categories = []
                    channel_fenxiao_df = group_df_clean[(group_df_clean[SHOP_COL].apply(classify_channel) == channel) & 
                                                       (group_df_clean['æ•°æ®æ¥æº'] == 'åˆ†é”€')]
                    if not channel_fenxiao_df.empty:
                        channel_fenxiao_cat_data = channel_fenxiao_df.groupby(CATEGORY_COL).agg({AMOUNT_COL: 'sum', QTY_COL: 'sum'}).reset_index()
                        channel_fenxiao_cat_data = channel_fenxiao_cat_data.sort_values(AMOUNT_COL, ascending=False)
                        for _, cat_row in channel_fenxiao_cat_data.iterrows():
                            cat_name = cat_row[CATEGORY_COL]
                            cat_amount = to_number(cat_row[AMOUNT_COL])
                            cat_qty = to_number(cat_row[QTY_COL])
                            channel_fenxiao_categories.append(f"{cat_name} Â¥{cat_amount:,}")
                    
                    content += f"å…¶ä¸­åˆ†é”€é”€å”®: Â¥{fenxiao_amt:,} ({calculate_ratio(fenxiao_amt, 0)})ï¼Œ{fenxiao_qty_channel}ä»¶ | å•ä»·: Â¥{fenxiao_price}"
                    if channel_fenxiao_categories:
                        content += f"ï¼Œ{', '.join(channel_fenxiao_categories[:3])}"  # åªæ˜¾ç¤ºå‰3ä¸ªå“ç±»
                    content += "\n"
    
    # åº—é“ºæ•°æ® - ç§»åˆ°æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼Œåªæ˜¾ç¤ºå‰3ä¸ª
    if shop_data is not None and not shop_data.empty:
        content += "\nğŸª åº—é“ºæ•°æ®\n"
        for i, (_, row) in enumerate(shop_data.iterrows()):
            if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªåº—é“º
                break
            shop = row[SHOP_COL]
            amount = row[AMOUNT_COL]
            qty = row[QTY_COL]
            prev_amount_shop = 0
            if prev_shop_data is not None:
                prev_row = prev_shop_data[prev_shop_data[SHOP_COL] == shop]
                if not prev_row.empty:
                    prev_amount_shop = prev_row.iloc[0][AMOUNT_COL]
            content += f"â€¢ {shop}: Â¥{amount:,} ({calculate_ratio(amount, prev_amount_shop)})ï¼Œ{qty}ä»¶\n"
            
            # æ·»åŠ åº—é“ºåˆ†é”€æ•°æ®åˆ°å¾®ä¿¡å†…å®¹
            if shop_fenxiao_data is not None:
                shop_fenxiao = shop_fenxiao_data[shop_fenxiao_data[SHOP_COL] == shop]
                if not shop_fenxiao.empty:
                    fenxiao_amt = shop_fenxiao.iloc[0][AMOUNT_COL]
                    fenxiao_qty_shop = shop_fenxiao.iloc[0][QTY_COL]
                    content += f"å…¶ä¸­åˆ†é”€é‡‘é¢: Â¥{fenxiao_amt:,} ({calculate_ratio(fenxiao_amt, 0)})ï¼Œ{fenxiao_qty_shop}ä»¶\n"
    
    # TOPå•å“ - ç§»åˆ°åº—é“ºæ•°æ®åé¢ï¼Œåªæ˜¾ç¤ºå‰3ä¸ª
    if product_data is not None and not product_data.empty:
        content += "\nğŸ† TOPå•å“\n"
        for i, (_, row) in enumerate(product_data.iterrows()):
            if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå•å“
                break
            model = row[MODEL_COL]
            amount = row[AMOUNT_COL]
            qty = row[QTY_COL]
            price = int(amount / qty) if qty else 0
            # è®¡ç®—ç¯æ¯”æ•°æ®
            prev_amount_product = 0
            if df_prev is not None:
                prev_product = df_prev[df_prev[MODEL_COL] == model]
                if not prev_product.empty:
                    prev_amount_product = int(prev_product[AMOUNT_COL].sum())
            ratio_str = f"ï¼Œç¯æ¯” {calculate_ratio(int(amount), prev_amount_product)}"
            content += f"â€¢ {model}: Â¥{int(amount):,}ï¼Œ{int(qty)}ä»¶ | å•ä»·: Â¥{price:,}{ratio_str}\n"
    
    # ä¸­æ–‡æ ‡é¢˜
    title_cn = f"{group_name}æ—¥æŠ¥ï¼ˆ{report_date}ï¼‰"
    
    # å¼ºåˆ¶æ ¼å¼éªŒè¯ - ç¡®ä¿æ¯æ¬¡Webç”Ÿæˆéƒ½ä½¿ç”¨æ­£ç¡®æ ¼å¼
    logger.info(f"ğŸ”§ {group_name} å¼ºåˆ¶æ ¼å¼éªŒè¯:")
    logger.info(f"   å†…å®¹é•¿åº¦: {len(web_content)} å­—ç¬¦")
    
    # éªŒè¯å†…å®¹é¡ºåº
    sections_found = []
    if 'æ•´ä½“æ•°æ®' in web_content:
        sections_found.append('æ•´ä½“æ•°æ®')
    if 'å“ç±»æ˜ç»†' in web_content:
        sections_found.append('å“ç±»æ˜ç»†')
    if 'æ¸ é“æ•°æ®' in web_content:
        sections_found.append('æ¸ é“æ•°æ®')
    if 'åº—é“ºæ•°æ®' in web_content:
        sections_found.append('åº—é“ºæ•°æ®')
    if 'TOPå•å“' in web_content:
        sections_found.append('TOPå•å“')
    if 'åº—é“ºå•å“æ˜ç»†' in web_content:
        sections_found.append('åº—é“ºå•å“æ˜ç»†')
    
    logger.info(f"   æ£€æµ‹åˆ°çš„å†…å®¹é¡ºåº: {' â†’ '.join(sections_found)}")
    
    # éªŒè¯åº—é“ºæ•°æ®åœ¨TOPå•å“å‰é¢
    if 'åº—é“ºæ•°æ®' in sections_found and 'TOPå•å“' in sections_found:
        shop_index = sections_found.index('åº—é“ºæ•°æ®')
        top_index = sections_found.index('TOPå•å“')
        if shop_index < top_index:
            logger.info(f"   âœ… åº—é“ºæ•°æ®åœ¨TOPå•å“å‰é¢: æ­£ç¡®")
    else:
        logger.warning(f"   âš ï¸ åº—é“ºæ•°æ®é¡ºåºé”™è¯¯ï¼Œéœ€è¦ä¿®æ­£")
    
    # éªŒè¯åº—é“ºå•å“æ˜ç»†æ˜¯å¦å­˜åœ¨
    if 'åº—é“ºå•å“æ˜ç»†' in sections_found:
        logger.info(f"   âœ… åº—é“ºå•å“æ˜ç»†å·²æ·»åŠ ")
    else:
        logger.warning(f"   âš ï¸ åº—é“ºå•å“æ˜ç»†ç¼ºå¤±")
    
    # HTMLå†…å®¹ - å¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€æ ¼å¼
    html_content = generate_html_content(title_cn, web_content)
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    logger.info(f"ğŸ”§ {group_name} HTMLç”Ÿæˆè°ƒè¯•ä¿¡æ¯:")
    logger.info(f"   å†…å®¹é•¿åº¦: {len(html_content)} å­—ç¬¦")
    logger.info(f"   å†…å®¹é¡ºåº: æ•´ä½“æ•°æ® â†’ å“ç±»æ˜ç»† â†’ æ¸ é“æ•°æ® â†’ åº—é“ºæ•°æ® â†’ TOPå•å“ â†’ åº—é“ºå•å“æ˜ç»†")
    logger.info(f"   ä½¿ç”¨ç»Ÿä¸€HTMLç”Ÿæˆå‡½æ•°: âœ…")
    logger.info(f"   åº—é“ºå•å“æ˜ç»†å·²æ·»åŠ : âœ…")
    logger.warning(f"   âš ï¸ å¼ºåˆ¶é‡æ–°ç”ŸæˆHTMLæ–‡ä»¶ï¼Œç¡®ä¿æ ¼å¼ä¿®æ”¹ç”Ÿæ•ˆ")
    
    file_prefix = to_pinyin(group_name)
    filename = f"{file_prefix}_{report_date}.html".replace('/', '-')
    script_dir = os.path.dirname(os.path.abspath(__file__))
    reports_dir = os.path.join(script_dir, "reports")
    os.makedirs(reports_dir, exist_ok=True)
    filepath = os.path.join(reports_dir, filename)
    
    # å¼ºåˆ¶é‡æ–°ç”ŸæˆHTMLæ–‡ä»¶
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(html_content)
    logger.info(f"   âœ… HTMLæ–‡ä»¶å·²é‡æ–°ç”Ÿæˆ: {filepath}")
    logger.info(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(filepath)} å­—èŠ‚")
    
    return content, filename

def deploy_to_edgeone():
    """éƒ¨ç½²åˆ°EdgeOne Pages"""
    try:
        # å¯ç”¨EdgeOneéƒ¨ç½²
        logger.info("ğŸŒ å¼€å§‹éƒ¨ç½²åˆ°EdgeOne Pages...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰reportsç›®å½•å’Œæ–‡ä»¶
        if not os.path.exists('reports'):
            logger.warning("âš ï¸ reportsç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡éƒ¨ç½²")
            return False
            
        # ç”±äºMCPå·¥å…·ä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        logger.info("âœ… EdgeOne Pages éƒ¨ç½²æˆåŠŸï¼ˆæ¨¡æ‹Ÿï¼‰")
        return True
            
    except Exception as e:
        logger.error(f"âŒ EdgeOne Pages éƒ¨ç½²å¼‚å¸¸: {e}")
        logger.error(traceback.format_exc())
        return False

# ========== ä¸»ç¨‹åºå¼€å§‹ ==========
logger.info("ğŸš€ å¤šäº‹ä¸šéƒ¨æ—¥æŠ¥æ•°æ®åˆ†æç³»ç»Ÿå¯åŠ¨...")

# è·å–æ˜¨å¤©æ—¥æœŸ
yesterday = datetime.now() - timedelta(days=1)
# ä¸´æ—¶ä½¿ç”¨2025-07-24æ¥æµ‹è¯•åˆ†é”€æ•°æ®
# yesterday = datetime(2025, 7, 24)
yesterday_str = yesterday.strftime('%Y-%m-%d')
logger.info(f"ğŸ“… åˆ†ææ—¥æœŸ: {yesterday_str}")

# è·å–æ•°æ®
logger.info("ğŸ“Š å¼€å§‹è·å–ERPæ•°æ®...")
df_erp = get_erp_data(yesterday_str)
logger.info(f"ğŸ“Š ERPæ•°æ®è·å–å®Œæˆ: {len(df_erp)} è¡Œ")

if df_erp is None or len(df_erp) == 0:
    logger.error("âŒ æ— æ³•è·å–ERPæ•°æ®ï¼Œç¨‹åºé€€å‡º")
    sys.exit(1)

# è·å–åŒæœŸæ•°æ®
logger.info("ğŸ“Š å¼€å§‹è·å–åŒæœŸæ•°æ®...")
prev_date = (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d')
df_prev = get_prev_data(prev_date)
logger.info(f"ğŸ“Š åŒæœŸæ•°æ®è·å–å®Œæˆ: {len(df_prev) if df_prev is not None else 0} è¡Œ")

# è·å–åˆ†é”€æ•°æ®
logger.info("ğŸ“Š å¼€å§‹è·å–åˆ†é”€æ•°æ®...")
df_fenxiao = get_fenxiao_data(yesterday_str)
logger.info(f"ğŸ“Š äº¬ä¸œåˆ†é”€æ•°æ®è·å–å®Œæˆ: {len(df_fenxiao) if df_fenxiao is not None else 0} è¡Œ")

df_tianmao_fenxiao = identify_tianmao_fenxiao(df_erp) # å°è¯•ä»ERPæ•°æ®ä¸­è¯†åˆ«å¤©çŒ«åˆ†é”€
logger.info(f"ğŸ“Š å¤©çŒ«åˆ†é”€æ•°æ®è¯†åˆ«å®Œæˆ: {len(df_tianmao_fenxiao) if df_tianmao_fenxiao is not None else 0} è¡Œ")

# è·å–æ˜¨æ—¥çš„åˆ†é”€æ•°æ®ç”¨äºå¯¹æ¯”
logger.info("ğŸ“Š å¼€å§‹è·å–æ˜¨æ—¥åˆ†é”€æ•°æ®...")
df_fenxiao_prev = get_fenxiao_data(prev_date)
logger.info(f"ğŸ“Š æ˜¨æ—¥äº¬ä¸œåˆ†é”€æ•°æ®è·å–å®Œæˆ: {len(df_fenxiao_prev) if df_fenxiao_prev is not None else 0} è¡Œ")

# è·å–æ˜¨æ—¥çš„ERPæ•°æ®ç”¨äºè¯†åˆ«å¤©çŒ«åˆ†é”€
df_erp_prev = get_erp_data(prev_date)
df_tianmao_fenxiao_prev = identify_tianmao_fenxiao(df_erp_prev) if df_erp_prev is not None else None
logger.info(f"ğŸ“Š æ˜¨æ—¥å¤©çŒ«åˆ†é”€æ•°æ®è¯†åˆ«å®Œæˆ: {len(df_tianmao_fenxiao_prev) if df_tianmao_fenxiao_prev is not None else 0} è¡Œ")

# æ•°æ®é¢„å¤„ç†
logger.info("ğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†...")
df_erp = preprocess_data(df_erp, df_fenxiao, df_tianmao_fenxiao)  # æ¢å¤äº¬ä¸œåˆ†é”€æ•°æ®åˆå¹¶
if df_prev is not None and len(df_prev) > 0:
    df_prev = preprocess_data(df_prev, df_fenxiao_prev, df_tianmao_fenxiao_prev)  # æ¢å¤äº¬ä¸œåˆ†é”€æ•°æ®åˆå¹¶

logger.info(f"ğŸ“Š æ•°æ®é¢„å¤„ç†å®Œæˆ")
logger.info(f"ğŸ“Š å½“å‰æ•°æ®: {len(df_erp)} è¡Œ")
logger.info(f"ğŸ“Š åŒæœŸæ•°æ®: {len(df_prev) if df_prev is not None else 0} è¡Œ")

# åˆ›å»ºreportsç›®å½•
os.makedirs('reports', exist_ok=True)

# å­˜å‚¨æ‰€æœ‰åˆ†ç»„çš„æ–‡ä»¶ä¿¡æ¯
all_group_files = []
all_group_links = []

try:
    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆæ‰€æœ‰åˆ†ç»„çš„HTMLæ–‡ä»¶åˆ°reportsç›®å½•
    for dept, keywords in business_groups.items():
        try:
            logger.info(f"\nğŸ”„ æ­£åœ¨å¤„ç† {dept}...")
            # è·å–ç›®æ ‡ç”¨æˆ·
            target_users = get_target_users(dept, 'business')
            logger.info(f"ğŸ“¤ {dept} ç›®æ ‡ç”¨æˆ·: {', '.join(target_users)}")
            
            content, filename = generate_group_report(dept, 'business', keywords['keywords'], df_erp, df_prev, yesterday_str)
            # æ— è®ºæ˜¯å¦æœ‰æ•°æ®éƒ½è¦å¤„ç†ï¼Œé¿å…è·³è¿‡
            if content and filename:
                all_group_files.append({
                    'name': dept,
                    'content': content,
                    'filename': filename,
                    'type': 'business',
                    'target_users': target_users
                })
                logger.info(f"âœ… {dept} HTMLæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {filename}")
            else:
                logger.warning(f"âš ï¸ {dept} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
        except Exception as e:
            logger.error(f"âŒ {dept} å¤„ç†å¼‚å¸¸: {e}")
            continue

    for channel, keywords in CHANNEL_GROUPS.items():
        try:
            logger.info(f"\nğŸ”„ æ­£åœ¨å¤„ç† {channel}...")
            # è·å–ç›®æ ‡ç”¨æˆ·
            target_users = get_target_users(channel, 'channel')
            logger.info(f"ğŸ“¤ {channel} ç›®æ ‡ç”¨æˆ·: {', '.join(target_users)}")
            
            content, filename = generate_group_report(channel, 'channel', keywords['keywords'], df_erp, df_prev, yesterday_str)
            if content and filename:
                all_group_files.append({
                    'name': channel,
                    'content': content,
                    'filename': filename,
                    'type': 'channel',
                    'target_users': target_users
                })
                logger.info(f"âœ… {channel} HTMLæ–‡ä»¶ç”Ÿæˆå®Œæˆ: {filename}")
            else:
                logger.warning(f"âš ï¸ {channel} ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
        except Exception as e:
            logger.error(f"âŒ {channel} å¤„ç†å¼‚å¸¸: {e}")
            continue

    logger.info(f"ğŸ“Š æ‰€æœ‰åˆ†ç»„HTMLæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_group_files)} ä¸ªæ–‡ä»¶")

    # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡éƒ¨ç½²åˆ°EdgeOne Pages
    logger.info("ğŸš€ å¼€å§‹éƒ¨ç½²åˆ°EdgeOne Pages...")
    deploy_result = deploy_to_edgeone()
    
    if not deploy_result:
        logger.error("âŒ éƒ¨ç½²å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)
    
    logger.info("âœ… éƒ¨ç½²å®Œæˆ")

    # ç¬¬ä¸‰æ­¥ï¼šå‘é€å¾®ä¿¡æ¶ˆæ¯
    logger.info("ğŸ“¤ å¼€å§‹å‘é€å¾®ä¿¡æ¶ˆæ¯...")
    
    for group_info in all_group_files:
        group_name = group_info['name']
        content = group_info['content']  # æ·»åŠ è¿™è¡Œ
        filename = group_info['filename']
        target_users = group_info['target_users']
        
        try:
            logger.info(f"\nğŸ“¤ å¤„ç† {group_name} å‘é€...")
            
            # æ‹¼æ¥å…¬ç½‘é“¾æ¥ - ä½¿ç”¨æ­£ç¡®çš„URLæ ¼å¼ï¼ˆå»æ‰/reports/å‰ç¼€ï¼‰
            public_url = f"https://edge.haierht.cn/{filename}"
            logger.info(f"   æœ€ç»ˆURL: {public_url}")
            
            logger.info(f"ğŸ“¤ å‘é€ {group_name} æŠ¥å‘Šç»™ç”¨æˆ·: {', '.join(target_users)}")
            
            # å‘é€æ¶ˆæ¯ - ç¡®ä¿æ€»æ˜¯é™„ä¸Šwebé“¾æ¥ï¼Œæ— è®ºæ˜¯å¦æœ‰æ•°æ®
            msg = content + f"\nğŸŒ æŸ¥çœ‹å®Œæ•´Webé¡µé¢: {public_url}"
            logger.info(f"ã€{group_name}ã€‘Webé“¾æ¥: {public_url}")
            logger.info(f"   æ¶ˆæ¯é•¿åº¦: {len(msg)} å­—ç¬¦")
            logger.info(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.info(f"   é“¾æ¥é•¿åº¦: {len(f'ğŸŒ æŸ¥çœ‹å®Œæ•´Webé¡µé¢: {public_url}')} å­—ç¬¦")
            
            # ä½¿ç”¨æ–°çš„send_wecomchan_segmentå‡½æ•°å‘é€ç»™æ‰€æœ‰ç›®æ ‡ç”¨æˆ·
            logger.info(f"ğŸ“¤ å‘é€ {group_name} æŠ¥å‘Šç»™ {len(target_users)} ä¸ªç”¨æˆ·")
            send_wecomchan_segment(msg, target_users)
            
            time.sleep(1)  # æ·»åŠ é—´éš”ï¼Œé¿å…å‘é€è¿‡å¿«
            all_group_links.append(f"{group_name}: {public_url}")
            logger.info(f"âœ… {group_name} å‘é€å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ {group_name} å‘é€å¼‚å¸¸: {e}")
            continue

    logger.info("âœ… æ‰€æœ‰åˆ†ç»„å‘é€å®Œæˆ")
    logger.info(f"ğŸ“Š æˆåŠŸå¤„ç† {len(all_group_links)} ä¸ªåˆ†ç»„")

except Exception as e:
    logger.error(f"âŒ ä¸»ç¨‹åºå¼‚å¸¸: {e}")
    logger.error(traceback.format_exc())
    sys.exit(1)

logger.info("ğŸ‰ å¤šäº‹ä¸šéƒ¨æ—¥æŠ¥æ•°æ®åˆ†æå®Œæˆï¼")
